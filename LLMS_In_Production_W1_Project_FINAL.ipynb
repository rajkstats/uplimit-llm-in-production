{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajkstats/uplimit-llm-in-production/blob/main/LLMS_In_Production_W1_Project_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataAlchemy Labs\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1jLQB9IRtp4cHnIxRSebSVcclKxNfdHCf\" />\n",
        "\n",
        "Welcome to the week 1 project for LLMs in Production. In this weeks project you are a founding Machine Learning Engineer on the team you and your team have narrowed down a life-changing product. You will be building and launching your groundbreaking SaaS product which is quite similar to a product already out in the market, [AI2sql](https://www.ai2sql.io/)."
      ],
      "metadata": {
        "id": "j8srXs0p12CE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![AI2sql Example](https://drive.google.com/uc?id=1mWOQs6OZmuCLui-auLosXsXGhwl2mbVg)\n",
        "\n",
        "You have done a significant amount of market research and are confident that this product will gather a cult-like following with a killer feature of being able to take any natural language query and output SQL code based on the query.\n",
        "\n",
        "Potential users of your application are:\n",
        "- **Project Managers**: They can use our tool to quickly derive important metrics for presentations or reports. Instead of relying on technical teams, they can directly query the database using natural language.\n",
        "- **Data Scientists**: Often dealing with complex data queries, data scientists can use our tool to streamline and debug intricate SQL statements, saving time and reducing the potential for error.\n",
        "- **Business Analysts and Non-Tech Professionals**: Anyone without deep SQL knowledge but needing to interact with the database can benefit immensely. They can easily convert their data needs into SQL queries without delving into the complexities of SQL syntax.\n",
        "\n",
        "There are many more features that you can work on in the future such as:\n",
        "1. NoSQL Code Generation\n",
        "2. SQL Syntax Checking\n",
        "3. Explaining SQL Code\n",
        "4. Optimizing SQL Code\n",
        "5. Formatting SQL Code\n",
        "\n",
        "But since this is an MVP and you want to laucnh as quickly as possible we will be just focussing on the use case of SQL Code Generation. In particular we will be supporting the use of **SQL** initially as its the most popular database out there in use currently based on the recent [StackOverflow Developer Survey 2023](https://survey.stackoverflow.co/2023/#section-most-popular-technologies-databases).\n",
        "\n",
        "\n",
        "\n",
        "Our final product will be a simple web application with a basic Q/A system that allows users to ask query about SQL and then get back SQL code that they can use for their application, think of it as a super basic version of the ChatGPT from OpenAI that I am sure everyone is quite familiar with.\n",
        "\n",
        "![ChatGPT OpenAI Example](https://drive.google.com/uc?id=1AGVZoxtWvBF6KLX0mkvpnU7mMEgaSraa)\n",
        "\n",
        "In the Week 1 Project we will be solidfying what we learnt throughout the week by building out this core functionality for the SQL Code Generation part of our application. Just like all Data Science projects we start off in a notebook environment to prototype things before having to bring them to production.\n",
        "\n",
        "Just like most LLM startups we will be leveraging OpenAI and their ChatGPT API initially for our product, in particular we will be using **gpt-4o-mini** which offers a good enough LLM that is quite fast!\n",
        "\n",
        "The main parts that we will be covering in the project are:\n",
        "1. [Using LLMs to Generate SQL Code](#scrollTo=HcGIbmPiM4o3)\n",
        "2. [Evaluating LLMs on SQL Code Generation](#scrollTo=pWz1btwOM7zr)\n",
        "3. [Validating LLM Outputs using Guardrails](#scrollTo=rPmPTQssNDUn)\n",
        "4. [Extra Credit: Optional Tasks](#scrollTo=UL-XI8PiNU1z&line=1&uniqifier=1)\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/10y3EFKr4S8TsiSp00y8_LTD-2UU8nCGS?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6lSc2PMKFPyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up OpenAI API Key"
      ],
      "metadata": {
        "id": "9MduGWtPIPTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "  padding: 10px;\n",
        "  border-radius: 5px;\n",
        "  background-color: #ffcccc;\n",
        "  border-left: 6px solid #ff0000;\n",
        "  margin-bottom: 20px;\">\n",
        "  \n",
        "  <strong>⚠️ Important Notice:</strong>\n",
        "  <p>Do not share or use this API Key outside of the context of the notebook exercises.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "q1v3j1VcIWFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uplimit has provisioned an OpenAI API Key for your projects. Please add this API Key to this assignment by clicking on the Security Key icon on the left hand tab of the Google Colab notebook and then add a new parameter value called `OPENAI_API_KEY`.\n",
        "\n",
        "\n",
        " Here you can provide the API key that you copied and this will not be part of your Google Colab account. You can also enable the toggle Notebook access - this will allow your notebook to have access to this API key.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1SfE-nNOQ3DfZpJN6mAnxuqVsMv-F1_Yp\" />\n",
        "\n",
        "**NOTE:** We are hardcoding an API key for [Guardrails.ai](https://www.guardrailsai.com/) which is used for downloading different types of LLM Guardrails, access is free regardless and we hardcode it as a convenience for all the students."
      ],
      "metadata": {
        "id": "RzMCOyXfIZCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the API Key has been setup, run the following code:"
      ],
      "metadata": {
        "id": "nMMB_lzHIdpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai guardrails-ai==0.5.0 deepeval langchain langchain-openai"
      ],
      "metadata": {
        "id": "vcOQFQC7jUU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ac9885-599c-4f3b-a16b-b0fe692ac633"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.3)\n",
            "Requirement already satisfied: guardrails-ai==0.5.0 in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: deepeval in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.17)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.1.25)\n",
            "Requirement already satisfied: coloredlogs<16.0.0,>=15.0.1 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (15.0.1)\n",
            "Requirement already satisfied: faker<26.0.0,>=25.2.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (25.9.2)\n",
            "Requirement already satisfied: griffe<0.37.0,>=0.36.9 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (0.36.9)\n",
            "Requirement already satisfied: guardrails-api-client>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (0.3.13)\n",
            "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (1.1.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (4.23.0)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (0.2.43)\n",
            "Requirement already satisfied: litellm<2.0.0,>=1.37.14 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (1.52.1)\n",
            "Requirement already satisfied: lxml<5.0.0,>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (4.9.4)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (3.8.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (1.24.0)\n",
            "Requirement already satisfied: pip>=22 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (24.1.2)\n",
            "Requirement already satisfied: pydantic<3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2.9.2)\n",
            "Requirement already satisfied: pydash<8.0.0,>=7.0.6 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (7.0.7)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2.9.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2.8.2)\n",
            "Requirement already satisfied: regex<2024.0.0,>=2023.10.3 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2023.12.25)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.6.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (13.9.3)\n",
            "Requirement already satisfied: rstr<4.0.0,>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (3.2.2)\n",
            "Requirement already satisfied: tenacity>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (0.8.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<0.10.0,>=0.9.0->guardrails-ai==0.5.0) (0.9.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-ai==0.5.0) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from deepeval) (7.4.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from deepeval) (3.20.3)\n",
            "Requirement already satisfied: sentry-sdk in /usr/local/lib/python3.10/dist-packages (from deepeval) (2.17.0)\n",
            "Requirement already satisfied: pytest-repeat in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.9.3)\n",
            "Requirement already satisfied: pytest-xdist in /usr/local/lib/python3.10/dist-packages (from deepeval) (3.6.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from deepeval) (2.10.1)\n",
            "Requirement already satisfied: ragas in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.2.4)\n",
            "Requirement already satisfied: docx2txt~=0.8 in /usr/local/lib/python3.10/dist-packages (from deepeval) (0.8)\n",
            "Requirement already satisfied: importlib-metadata>=6.0.2 in /usr/local/lib/python3.10/dist-packages (from deepeval) (7.0.0)\n",
            "Requirement already satisfied: opentelemetry-api~=1.24.0 in /usr/local/lib/python3.10/dist-packages (from deepeval) (1.24.0)\n",
            "Requirement already satisfied: grpcio~=1.63.0 in /usr/local/lib/python3.10/dist-packages (from deepeval) (1.63.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs<16.0.0,>=15.0.1->guardrails-ai==0.5.0) (10.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.10/dist-packages (from griffe<0.37.0,>=0.36.9->guardrails-ai==0.5.0) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from guardrails-api-client>=0.3.8->guardrails-ai==0.5.0) (75.1.0)\n",
            "Requirement already satisfied: urllib3<2.1.0,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from guardrails-api-client>=0.3.8->guardrails-ai==0.5.0) (2.0.7)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.0.2->deepeval) (3.20.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (0.20.0)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (3.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (0.1.4)\n",
            "Requirement already satisfied: rfc3987 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (1.3.8)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (24.8.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1->guardrails-ai==0.5.0) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1->guardrails-ai==0.5.0) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (8.1.7)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (3.1.4)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (1.0.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (0.19.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->guardrails-ai==0.5.0) (1.4.2)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api~=1.24.0->deepeval) (1.2.14)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai==0.5.0) (1.65.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai==0.5.0) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.24.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc<2.0.0,>=1.24.0->guardrails-ai==0.5.0) (1.24.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.45b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<2.0.0,>=1.24.0->guardrails-ai==0.5.0) (0.45b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=2.0.0->guardrails-ai==0.5.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0,>=2.0.0->guardrails-ai==0.5.0) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.2->guardrails-ai==0.5.0) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->guardrails-ai==0.5.0) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.6.0->guardrails-ai==0.5.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.6.0->guardrails-ai==0.5.0) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<0.10.0,>=0.9.0->guardrails-ai==0.5.0) (1.5.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval) (1.5.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->deepeval) (2.0.2)\n",
            "Requirement already satisfied: execnet>=2.1 in /usr/local/lib/python3.10/dist-packages (from pytest-xdist->deepeval) (2.1.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (3.1.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (0.2.18)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (1.4.4)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from ragas->deepeval) (0.3.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api~=1.24.0->deepeval) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm<2.0.0,>=1.37.14->guardrails-ai==0.5.0) (3.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.6.0->guardrails-ai==0.5.0) (0.1.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->ragas->deepeval) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas->deepeval) (0.24.7)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas->deepeval) (0.6.7)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format]<5.0.0,>=4.22.0->guardrails-ai==0.5.0) (2.9.0.20241003)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval) (0.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas->deepeval) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas->deepeval) (2024.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas->deepeval) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('UL_OPENAI_API_KEY')\n",
        "os.environ[\"GUARDRAILS_TOKEN\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDEwNzI3MDE2NDYwNDM4MTg2OTU4OSIsImFwaUtleUlkIjoiYjAzNDVkNGEtZDhjNy00OThmLWIwZGYtZWI3ZTY0MzMwNTJkIiwiaWF0IjoxNzMwMTQ2ODc0LCJleHAiOjE3Mzc5MjI4NzR9.1J84O3pT_KlWTkGjV4zfBMChqYow868A-XTsePwiZ_Q\""
      ],
      "metadata": {
        "id": "OFBxDLxcIViM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Using LLMs to Generate SQL Code\n",
        "\n",
        "## SQL Code Generation\n",
        "\n",
        "SQL is one of the most powerful programming languages out there and is the main way in which we communicate with databases. Without it backend engineers would be unable to store and structure data to be easily queried to return to a frontend application and data scientists would be unable to track key metrics about their experiments over time.\n",
        "\n",
        "SQL is the language used for interacting with databases, while SQL is a specific database system that understands and uses SQL for database operations. Each database system, including SQL, implements SQL with some variations and adds its own proprietary extensions to the standard SQL language.\n",
        "\n",
        "Here is an example SQL statement:\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "    u.name,\n",
        "    u.email,\n",
        "    COUNT(o.order_id) AS total_orders,\n",
        "    SUM(o.amount) AS total_amount_spent\n",
        "FROM\n",
        "    users u\n",
        "JOIN\n",
        "    orders o ON u.user_id = o.user_id\n",
        "WHERE\n",
        "    o.order_date >= (NOW() - INTERVAL '1 year')\n",
        "GROUP BY\n",
        "    u.user_id\n",
        "HAVING\n",
        "    COUNT(o.order_id) > 5\n",
        "ORDER BY\n",
        "    total_amount_spent DESC;\n",
        "```\n",
        "\n",
        "LLM's such as ChatGPT from OpenAI are trained on all kinds of data across the internet and in particular they are trained on code across many different programming languages.\n",
        "\n",
        "\n",
        "![Text2SQL](https://drive.google.com/uc?id=17IDU11L_JdPIfMWvDa6li_5oOyGgWriX)\n",
        "\n",
        "What we will be performing is asking the LLM a Question related to a SQL query in order to create SQL code that we can then run.\n",
        "\n",
        "\n",
        "During OpenAI DevDay, there was a fantastic breakout section about **\"[A Survey of Techniques for Maximizing LLM Performance](https://youtu.be/ahnGLM-RC1Y?si=rifoSUxFgvgliFtT)\"** which we recommend all of you go through. In that talk, the speakers highlighted how we should always be starting off with Prompt Engineering when building LLM applications. Once we understand what our LLM application lacks can we only try more complex techniques such as RAG or Fine-tuning or both. With that spirit in mind let's try out some basic Prompt Engineering using ChatGPT.\n",
        "\n",
        "![Optimization Flow from OpenAI](https://drive.google.com/uc?id=1kcguLT8KYBmDypGB0fVyeT8YfrCiiMx_)"
      ],
      "metadata": {
        "id": "HcGIbmPiM4o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import LLMResult, HumanMessage, Generation\n",
        "\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=MODEL_NAME,\n",
        "    temperature=0.0\n",
        ")"
      ],
      "metadata": {
        "id": "82LGwE57Zc4a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Create a Baseline Zero-Shot Prompt\n",
        "\n",
        "Create a Zero-Shot Prompt that we can use in this situation, we have provided you with a simple prompt to try out. A **Zero-Shot prompt** is where we are asking the model to perform a task it has never seen explicty during training without any additional examples.\n",
        "\n",
        "When working on the prompt or improving the prompt remember a couple of things:\n",
        "\n",
        "1. Make sure to use clear instructions\n",
        "2. Try splitting tasks into simpler subtasks\n",
        "3. Give the model time to “think”\n",
        "\n",
        "![How to Prompt Engineer](https://drive.google.com/uc?id=1iSZ6XvD5eMiRSQBUT0DFEDA-1qc0qibv)\n"
      ],
      "metadata": {
        "id": "RE5HXufjHF8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your prompt here, make sure to use\n",
        "# {query} to inject a query into the prompt.\n",
        "\n",
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"\n",
        "You are an expert SQL developer tasked with translating natural language queries into precise SQL statements. Follow these steps:\n",
        "\n",
        "Analyze the query:\n",
        "- Identify the main tables involved\n",
        "- Determine the required operations (SELECT, JOIN, GROUP BY,ORDER BY etc.)\n",
        "- Note any conditions or constraints\n",
        "\n",
        "ASSUMPTIONS:\n",
        "- List any assumptions about table structures and relationships\n",
        "- If column names aren't specified, use common conventional names\n",
        "- Document any assumed data types\n",
        "\n",
        "SQL Translation:\n",
        "\n",
        "Write the SQL query following these guidelines:\n",
        "- Use proper SQL syntax and formatting\n",
        "- Include appropriate table aliases where needed\n",
        "- Add comments to explain complex logic\n",
        "- Consider performance implications\n",
        "\n",
        "Explanation:\n",
        "- Provide a brief explanation of how the query works\n",
        "\n",
        "Here is the query to translate: {query}\n",
        "\n",
        "Please format your response as follows:\n",
        "\n",
        "SQL QUERY:\n",
        "[Your SQL query here]\n",
        "\n",
        "Assumtions:\n",
        "[List your assumptions here]\n",
        "\n",
        "Explanation:\n",
        "[Brief explanation of the query logic]\n",
        "\"\"\"\n",
        "\n",
        "# Try your own query\n",
        "SAMPLE_QUERY = \"Select the name of the employee who has the highest salary\"\n",
        "\n",
        "prompt = ZERO_SHOT_PROMPT_TEMPLATE.format(\n",
        "    query=SAMPLE_QUERY\n",
        ")\n",
        "\n",
        "result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "\n",
        "# # Do try to investigate the generations from LangChain to understand what is generated\n",
        "print(result.generations[0][0].text)"
      ],
      "metadata": {
        "id": "xcLCb77HbZ1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d82d77ee-5fb5-40e3-d96e-38bb3ee8951b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQL QUERY:\n",
            "```sql\n",
            "SELECT e.name\n",
            "FROM employees e\n",
            "WHERE e.salary = (SELECT MAX(salary) FROM employees)\n",
            "```\n",
            "\n",
            "Assumptions:\n",
            "1. There is a table named `employees` that contains employee data.\n",
            "2. The `employees` table has at least the following columns:\n",
            "   - `name` (data type: VARCHAR or similar)\n",
            "   - `salary` (data type: DECIMAL or FLOAT)\n",
            "3. The `salary` column contains numeric values representing the employee's salary.\n",
            "4. It is assumed that there is at least one employee in the `employees` table.\n",
            "\n",
            "Explanation:\n",
            "The query retrieves the name of the employee with the highest salary. It does this by using a subquery to find the maximum salary from the `employees` table. The outer query then selects the name of the employee whose salary matches this maximum value. This approach ensures that if there are multiple employees with the same highest salary, all of their names will be returned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job! We have generated our very first SQL code using an LLM!\n",
        "\n",
        "An improvement we can perform on this is using a **Few-Shot prompt** where we include a couple of examples for the LLM to understand what we are trying to do. Doing so should result in better results as the LLM has some reference examples about what we are trying to do.\n",
        "\n",
        "**BUT** remember *we need to evaluate our LLM* prompt before we can even think about experimenting on it otherwise how would we even know if this new prompt is even better in the first place?"
      ],
      "metadata": {
        "id": "ys1WKl5JdIoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Evaluating LLMs on SQL Code Generation\n",
        "\n",
        "Evaluating Text-to-SQL might seem like an easy tasks to evaluate as all we need in order to generate our Golden Dataset would be a list of natural language queries and their respective SQL query but there are some intricacies with it that will prove to be challenging.\n",
        "\n",
        "We will need some way to check for **equivalence** to the **gold** query, which is where prior mentioned intricacies will start to pop up, but we will circle back to them later on. With that, we can start off with looking for an exact match between the LLM response and the golden query. For our initial metric we will be using a very simple metric, [exact match](https://huggingface.co/spaces/evaluate-metric/exact_match).\n",
        "\n",
        "![Basic Exact Match LLM Evaluator](https://drive.google.com/uc?id=1yIygL2zNC4JZ02dMBtC5HyNa0_ujIOOv)\n",
        "\n",
        "With a Golden Dataset which we can hand generate using our domain expertise, we can build up an evaluation dataset to run on every iteration to improve our application. Using the exact match metric we can derive an accuracy score of our LLM-application that we can keep track of and use in order to compare the performance across different iterations overtime.\n",
        "\n",
        "Here is an example of what our dataset can look like:\n",
        "\n",
        "![Example Golden Dataset](https://drive.google.com/uc?id=12D8C2juErtOzvKVLaKL_njxlceFJcbTu)\n",
        "\n",
        "**NOTE:** If you noticed, there is a **complexity** column on the SQL code used in the example which we can try incorporating as an optional tasks in order to improve the quality of our results across different fine-grained categories which is paramount in ensuring our LLM SQL Code Generator is more robust. Ideally we should have an even distribution across all these additional categories that we add on, where we can then calculate metrics such as **Accuracy@HardComplexity** and so on.\n",
        "\n",
        "We will provide you with a basic evaluation dataset to start off with, which we can further improve on using the process laid out below using [deepeval](https://github.com/confident-ai/deepeval):\n",
        "\n",
        "![Evaluation Process](https://drive.google.com/uc?id=1KC_gMvU3PeflEm0N1Co8_YnT46lCvMLU)\n",
        "<!-- 1. Load a Golden Test set for the students in a JSONL file with 20-30 examples\n",
        "2. TODO: Create a custom deep-eval evaluator for SQL code where we check if the output of the SQL code exactly matches the output from the LLM.\n",
        "3. TODO: Run evaluation on the golden test set.\n",
        "4. TODO: Try different prompts and see if they improve the evaluation metric\n",
        "5. Checking for an exact match is not the best way to go about it as the same query could have multiple ways to do it. Let's try using G-Eval where we use another more powerful LLM.\n",
        "6. TODO: Evaluate using G-Eval\n",
        "7. Talk about how in a Production system we should be running evaluation just like how we run automated tests, via CI/CD pipelines. -->"
      ],
      "metadata": {
        "id": "pWz1btwOM7zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASIC_GROUND_TRUTH_DATASET = [\n",
        "  {\n",
        "    \"Query\": \"Show the total number of employees.\",\n",
        "    \"Ground Truth\": \"SELECT COUNT(*) AS total_employees FROM employees;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Find the highest salary in the Finance department.\",\n",
        "    \"Ground Truth\": \"SELECT MAX(salary) FROM employees WHERE department = 'Finance';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Get the average age of all managers.\",\n",
        "    \"Ground Truth\": \"SELECT AVG(age) FROM employees WHERE position = 'Manager';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List the names and emails of staff in the IT department.\",\n",
        "    \"Ground Truth\": \"SELECT name, email FROM employees WHERE department = 'IT';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"What are the titles of the top 5 selling books?\",\n",
        "    \"Ground Truth\": \"SELECT title FROM books ORDER BY sales DESC LIMIT 5;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Which product has the least quantity in stock?\",\n",
        "    \"Ground Truth\": \"SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Display the second highest salary in the organization.\",\n",
        "    \"Ground Truth\": \"SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List employees who joined after 2015 and work in the Sales department.\",\n",
        "    \"Ground Truth\": \"SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Find the average order value for each customer.\",\n",
        "    \"Ground Truth\": \"SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Show departments that have more than 10 employees.\",\n",
        "    \"Ground Truth\": \"SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List all products that have never been ordered.\",\n",
        "    \"Ground Truth\": \"SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Which customers have spent more than $1000 in total?\",\n",
        "    \"Ground Truth\": \"SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Show the total number of orders placed each day last week.\",\n",
        "    \"Ground Truth\": \"SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"List the names of employees who do not manage anyone.\",\n",
        "    \"Ground Truth\": \"SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);\"\n",
        "  },\n",
        "  {\n",
        "    \"Query\": \"Display the department that has the highest average employee salary.\",\n",
        "    \"Ground Truth\": \"SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\"\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "dKEiy2NmToCC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(BASIC_GROUND_TRUTH_DATASET)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JSrkIWZETx2W",
        "outputId": "7cc2896d-8332-4584-fb6c-9093278af95c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Query  \\\n",
              "0                Show the total number of employees.   \n",
              "1  Find the highest salary in the Finance departm...   \n",
              "2               Get the average age of all managers.   \n",
              "3  List the names and emails of staff in the IT d...   \n",
              "4    What are the titles of the top 5 selling books?   \n",
              "\n",
              "                                        Ground Truth  \n",
              "0  SELECT COUNT(*) AS total_employees FROM employ...  \n",
              "1  SELECT MAX(salary) FROM employees WHERE depart...  \n",
              "2  SELECT AVG(age) FROM employees WHERE position ...  \n",
              "3  SELECT name, email FROM employees WHERE depart...  \n",
              "4  SELECT title FROM books ORDER BY sales DESC LI...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5db0f8de-2673-468d-88b3-34ded70b6b00\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>Ground Truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Show the total number of employees.</td>\n",
              "      <td>SELECT COUNT(*) AS total_employees FROM employ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Find the highest salary in the Finance departm...</td>\n",
              "      <td>SELECT MAX(salary) FROM employees WHERE depart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Get the average age of all managers.</td>\n",
              "      <td>SELECT AVG(age) FROM employees WHERE position ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>List the names and emails of staff in the IT d...</td>\n",
              "      <td>SELECT name, email FROM employees WHERE depart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the titles of the top 5 selling books?</td>\n",
              "      <td>SELECT title FROM books ORDER BY sales DESC LI...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5db0f8de-2673-468d-88b3-34ded70b6b00')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5db0f8de-2673-468d-88b3-34ded70b6b00 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5db0f8de-2673-468d-88b3-34ded70b6b00');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-11e94dd8-75f8-4603-b5f7-7ef436230e43\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-11e94dd8-75f8-4603-b5f7-7ef436230e43')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-11e94dd8-75f8-4603-b5f7-7ef436230e43 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Show departments that have more than 10 employees.\",\n          \"Which customers have spent more than $1000 in total?\",\n          \"Show the total number of employees.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ground Truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\",\n          \"SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\",\n          \"SELECT COUNT(*) AS total_employees FROM employees;\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, lets see how our LLM and Prompt are able to perform on the entire dataset by running it through and evaluating its performance. This is known as performing a LLM Task Evaluation which is different than LLM Model Evals. Task evaluation cares about the performance of the LLM on a specific task whereas Model evals care about the general performance of the LLM across a variety of tasks, benchmarks such as the [HellaSwag](https://rowanzellers.com/hellaswag/) dataset is an example of a Model Evals and Text2SQL accuracy like we are doing in this project is an example of task evaluation.\n",
        "\n",
        "We will start off with traditional NLP metrics that compare the output with the ground truth can derive a score based on the exact/similarity of the two. These metrics are simple to use and serve as a good place to start off when evaluating LLMs.\n",
        "\n",
        "Firstly, let's start off with the exact match metric which very simply just compares the output with the ground truth and checks whether its an exact match or not, deepeval implements these metrics within the [Scorer module](https://github.com/confident-ai/deepeval/blob/4b3ceed20993232331550798fe0a8f1bf2605594/deepeval/scorer/scorer.py#L7).\n",
        "\n",
        "deepeval makes it easy to create our own metrics by inheriting the BaseMetric class, you can read more about creating custom metrics [here](https://docs.confident-ai.com/docs/metrics-custom)."
      ],
      "metadata": {
        "id": "qBOyfpn54-1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import BaseMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.scorer import Scorer\n",
        "\n",
        "\n",
        "class ExactMatchMetric(BaseMetric):\n",
        "    def __init__(self, threshold: float = 0.0, async_mode: bool = True):\n",
        "        self.threshold = threshold\n",
        "        self.async_mode = async_mode\n",
        "\n",
        "    def _base_measure(self, test_case: LLMTestCase):\n",
        "        self.success = Scorer.exact_match_score(\n",
        "            test_case.actual_output, test_case.expected_output\n",
        "        )\n",
        "        if self.success:\n",
        "            self.score = 1\n",
        "        else:\n",
        "            self.score = 0\n",
        "        return self.score\n",
        "\n",
        "    def measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    async def a_measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    def is_successful(self):\n",
        "        return bool(self.success)\n",
        "\n",
        "    @property\n",
        "    def __name__(self):\n",
        "        return \"ExactMatch\""
      ],
      "metadata": {
        "id": "PPOcKnpcUlD0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create our first test case!\n",
        "\n",
        "test_case_input = df.head(1)\n",
        "test_case_query = test_case_input['Query'].squeeze()\n",
        "test_case_expected_output = test_case_input['Ground Truth'].squeeze()"
      ],
      "metadata": {
        "id": "5apnNHGFYvno"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Here is our query: {test_case_query}\")\n",
        "print(f\"Here is our ground truth: {test_case_expected_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrLlDUKMb7oq",
        "outputId": "b5eccb33-fb93-4108-ab04-1c879ff896a3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is our query: Show the total number of employees.\n",
            "Here is our ground truth: SELECT COUNT(*) AS total_employees FROM employees;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"\n",
        "Generate a valid SQL query for the following natural language instruction:\n",
        "\n",
        "${query}\n",
        "\n",
        "Only generate SQL code and nothing else.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ZERO_SHOT_PROMPT_TEMPLATE.format(\n",
        "    query=test_case_query\n",
        ")\n",
        "\n",
        "result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "\n",
        "test_case_actual_output = result.generations[0][0].text"
      ],
      "metadata": {
        "id": "7twvbNfOaMaW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Here is what our model generated: {test_case_actual_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue1bMG-ubEpt",
        "outputId": "78fbcfd4-cc86-4fd0-f74e-1a397ec9450b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is what our model generated: ```sql\n",
            "SELECT COUNT(*) AS total_employees FROM employees;\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "test_case = LLMTestCase(\n",
        "  input=test_case_input,\n",
        "  actual_output=\"SELECT COUNT(*) AS total_employees FROM employees;\",\n",
        "  expected_output=test_case_expected_output\n",
        ")"
      ],
      "metadata": {
        "id": "9FQdZAxXZrQ3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exact_match_metric = ExactMatchMetric()\n",
        "exact_match_metric.measure(test_case)"
      ],
      "metadata": {
        "id": "9oQx25b7ctYh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Test case: {exact_match_metric.is_successful()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM2s34m-dBKW",
        "outputId": "f208352e-b011-48a9-a6c3-91a097c479ce"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test case: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE:** If your prompt still fails and produces a different SQL code such as *\"SELECT COUNT(*) AS total_employees FROM employees;\"* feel free to update the ground truth to get it to pass in the meantime. This can happen as code correctness is not a simple task due to the fact that different implementations can still solve the query which is something we invite you to address via the optional tasks for this week.\n",
        "\n",
        "\n",
        "Great job! We are able to see that our LLM and Prompt are able to answer the test case correctly, but that is just a basic example that we have. Next let us run it on the entire dataset that we have created.\n",
        "\n",
        "## TODO: Create a deepeval dataset and evalute our LLM and Prompt on it\n",
        "\n",
        "There is a slight difference between what we have been learning about Golden Datasets in general and how Golden Datasets work within deepeval. When we talk about a Golden Dataset as per the [course materials here](https://uplimit.com/course/llms-in-production/v2/module/evaluating-llms#corise_clrnjsvpd001m2e6iogarwdpn) we mention that its similar to a held-out test set meaning that we have data to predict on alongside their ground truth labels in order to derive evaluation metrics.\n",
        "\n",
        "In deepeval, we will need to create an Evaluation Dataset instead where an Evaluation Dataset is built from LLMTestCase's and/or Golden's. Remember that the `actual_output` comes from running our LLM against the `input` as we did above!\n",
        "```python\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "first_test_case = LLMTestCase(input=\"...\", actual_output=\"...\")\n",
        "second_test_case = LLMTestCase(input=\"...\", actual_output=\"...\")\n",
        "\n",
        "test_cases = [first_test_case, second_test_case]\n",
        "dataset = EvaluationDataset(test_cases=test_cases)\n",
        "```\n",
        "The definition of Golden's within deepeval is documented [here](https://docs.confident-ai.com/docs/confident-ai-manage-datasets#what-is-a-golden):\n",
        "```\n",
        "A \"Golden\" is what makes up an evaluation dataset and is very similar to a test case in deepeval, but they:\n",
        "\n",
        "    - do not require an actual_output, so whilst test cases are always ready for evaluation, a golden isn't.\n",
        "    - only exists within an EvaluationDataset(), while test cases can be defined anywhere.\n",
        "    - contains an extra additional_metadata field, which is a dictionary you can define on Confident. Allows you to do some extra preprocessing on your dataset (eg., generating a custom LLM actual_output based on some variables in additional_metadata) before evaluation.\n",
        "\n",
        "We introduced the concept of goldens because it allows you to create evaluation datasets on Confident without needing pre-computed actual_outputs. This is especially helpful if you are looking to generate responses from your LLM application at evaluation time.\n",
        "```\n",
        "With our EvaluationDataset created we can then run our LLM through it in order to derive the final score. For example, the following does so without using Pytest although its recommended that you treat this process as a CI/CD process that runs alongside your Pytest suite for your actual application.\n",
        "\n",
        "```python\n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "dataset = EvaluationDataset(test_cases=[...])\n",
        "hallucination_metric = HallucinationMetric(threshold=0.3, context=[\"...\"])\n",
        "answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
        "\n",
        "dataset.evaluate([hallucination_metric, answer_relevancy_metric])\n",
        "\n",
        "# You can also call the evaluate() function directly\n",
        "evaluate(dataset, [hallucination_metric, answer_relevancy_metric])\n",
        "```\n",
        "An example output of the evaluation would look like this:\n",
        "```\n",
        "======================================================================\n",
        "\n",
        "Metrics Summary\n",
        "\n",
        "  - ✅ Answer Relevancy (score: 1, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 1.00 because the response perfectly addresses the question about operating hours without any irrelevant information. Great job!, error: None)\n",
        "  - ✅ Bias (score: 0, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 0.00 because the output maintains neutrality and objectivity, as evidenced by the absence of any cited biased phrases or issues., error: None)\n",
        "\n",
        "For test case:\n",
        "\n",
        "  - input: What are your operating hours?\n",
        "  - actual output: ...\n",
        "  - expected output: None\n",
        "  - context: ['Our company operates from 10 AM to 6 PM, Monday to Friday.', 'We are closed on weekends and public holidays.', 'Our customer service is available 24/7.']\n",
        "  - retrieval context: None\n",
        "\n",
        "======================================================================\n",
        "\n",
        "Metrics Summary\n",
        "\n",
        "  - ✅ Answer Relevancy (score: 1, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 1.00 because the response accurately addresses the question about free shipping without any irrelevant information. Great job on maintaining focus!, error: None)\n",
        "  - ✅ Bias (score: 0, threshold: 0.5, strict: False, evaluation model: gpt-4-turbo, reason: The score is 0.00 because the actual output perfectly demonstrates neutrality and objectivity, without any indication of bias., error: None)\n",
        "\n",
        "For test case:\n",
        "\n",
        "  - input: Do you offer free shipping?\n",
        "  - actual output: ...\n",
        "  - expected output: Yes, we offer free shipping on orders over $50.\n",
        "  - context: None\n",
        "  - retrieval context: None\n",
        "\n",
        "======================================================================\n",
        "```\n",
        "One thing to note is that the metrics are run on a single (query, ground_truth) instead across the entire dataset meaning that we lack the ability to have a singular number to compare across different evaluation runs like traditional machine learning where you would for example have a accuracy score for the test set.\n",
        "\n",
        "Let us introduce the concept of pass rates which are similar to accuracy across the entire dataset but its just meant to show how many instances for a metric in the dataset passed a evaluation metric check.\n",
        "\n",
        "With this you could enforce a strict pass rate on deployments much like how we can enforce Pytest code coverage to be off a certain %.\n",
        "\n",
        "deepeval supports this via the `aggregate_metric_pass_rates()` on the results of the evaluation give us pass rates across each metric that we use:\n",
        "\n",
        "```python\n",
        "from deepeval.evaluate import aggregate_metric_pass_rates\n",
        "\n",
        "evaluation_results = evaluate(dataset, [hallucination_metric, answer_relevancy_metric])\n",
        "aggregate_metric_pass_rates(evaluation_results.test_results)\n",
        "```\n",
        "\n",
        "This should output the following where we get the pass rates across each metric and a mapping storing the pass rates across each metric which we can use to store and compare across different evaluation runs:\n",
        "```\n",
        "======================================================================\n",
        "\n",
        "Aggregate Metric Pass Rates\n",
        "\n",
        "AnswerRelevancyMetric: 100.00% pass rate\n",
        "BiasMetric: 100.00% pass rate\n",
        "\n",
        "======================================================================\n",
        "\n",
        "{'AnswerRelevancyMetric': 1.0, 'BiasMetric': 1.0}\n",
        "```\n",
        "\n",
        "Now with this you can apply it to our dataset:\n",
        "1. Create an [EvaluationDataset](https://docs.confident-ai.com/docs/evaluation-datasets#create-an-evaluation-dataset) using deepeval.\n",
        "2. Run the evaluation on the Golden Dataset without Pytest, refer to [this](https://docs.confident-ai.com/docs/evaluation-introduction#evaluating-without-pytest)."
      ],
      "metadata": {
        "id": "n-A23MUVfNhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WryCdfqosN_R",
        "outputId": "8a3a1316-e83a-4dfd-f587-3e3fe00ac792"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Query  \\\n",
              "0                Show the total number of employees.   \n",
              "1  Find the highest salary in the Finance departm...   \n",
              "2               Get the average age of all managers.   \n",
              "3  List the names and emails of staff in the IT d...   \n",
              "4    What are the titles of the top 5 selling books?   \n",
              "\n",
              "                                        Ground Truth  \n",
              "0  SELECT COUNT(*) AS total_employees FROM employ...  \n",
              "1  SELECT MAX(salary) FROM employees WHERE depart...  \n",
              "2  SELECT AVG(age) FROM employees WHERE position ...  \n",
              "3  SELECT name, email FROM employees WHERE depart...  \n",
              "4  SELECT title FROM books ORDER BY sales DESC LI...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b17bc139-5527-4f3c-aad8-cca38aa51eb5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>Ground Truth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Show the total number of employees.</td>\n",
              "      <td>SELECT COUNT(*) AS total_employees FROM employ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Find the highest salary in the Finance departm...</td>\n",
              "      <td>SELECT MAX(salary) FROM employees WHERE depart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Get the average age of all managers.</td>\n",
              "      <td>SELECT AVG(age) FROM employees WHERE position ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>List the names and emails of staff in the IT d...</td>\n",
              "      <td>SELECT name, email FROM employees WHERE depart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the titles of the top 5 selling books?</td>\n",
              "      <td>SELECT title FROM books ORDER BY sales DESC LI...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b17bc139-5527-4f3c-aad8-cca38aa51eb5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b17bc139-5527-4f3c-aad8-cca38aa51eb5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b17bc139-5527-4f3c-aad8-cca38aa51eb5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a15a8a39-a88f-4a6f-90e2-c419baf6b213\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a15a8a39-a88f-4a6f-90e2-c419baf6b213')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a15a8a39-a88f-4a6f-90e2-c419baf6b213 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"Show departments that have more than 10 employees.\",\n          \"Which customers have spent more than $1000 in total?\",\n          \"Show the total number of employees.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ground Truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\",\n          \"SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\",\n          \"SELECT COUNT(*) AS total_employees FROM employees;\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval import evaluate\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "from deepeval.metrics import (\n",
        "    HallucinationMetric,\n",
        "    AnswerRelevancyMetric)\n",
        "from deepeval.evaluate import aggregate_metric_pass_rates\n",
        "\n",
        "\n",
        "class ExactMatchMetric(BaseMetric):\n",
        "    def __init__(self, threshold=0.0, async_mode=True):\n",
        "        self.threshold = threshold\n",
        "        self.async_mode = async_mode\n",
        "\n",
        "    def _base_measure(self, test_case: LLMTestCase):\n",
        "        self.success = Scorer.exact_match_score(\n",
        "            test_case.actual_output,\n",
        "            test_case.expected_output\n",
        "        )\n",
        "        if self.success:\n",
        "            self.score = 1\n",
        "        else:\n",
        "            self.score = 0\n",
        "        return self.score\n",
        "\n",
        "    def measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    async def a_measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    def is_successful(self):\n",
        "        return bool(self.success)\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return \"ExactMatch\"\n",
        "\n",
        "def create_evaluation_dataset(df, llm, prompt_template):\n",
        "    test_cases = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "\n",
        "        # Formatting prompt with the query from df\n",
        "        prompt = prompt_template.format(query=row['Query'])\n",
        "\n",
        "        # translating text to sql using llm\n",
        "        result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "\n",
        "        actual_output = result.generations[0][0].text\n",
        "\n",
        "        # Creating test cases and appending\n",
        "        test_case = LLMTestCase(\n",
        "            input=row['Query'],\n",
        "            actual_output=actual_output,\n",
        "            expected_output=row['Ground Truth'],\n",
        "            context=[row['Query']]\n",
        "        )\n",
        "        test_cases.append(test_case)\n",
        "\n",
        "    return EvaluationDataset(test_cases=test_cases)\n",
        "\n",
        "def run_evaluation(df, llm, prompt_template):\n",
        "\n",
        "    print(\"Creating evaluation dataset\")\n",
        "    dataset = create_evaluation_dataset(df, llm, prompt_template)\n",
        "\n",
        "    print(\"\\n Running evaluation metrics\")\n",
        "    # Checks if the generated SQL exactly matches the ground truth, useful for basic validation\n",
        "    exact_match_metric = ExactMatchMetric()\n",
        "\n",
        "    # Checks if the model generates non-existent tables/columns, using 0.3 which means say allowing some minor deviations in results\n",
        "    hallucination_metric = HallucinationMetric(threshold=0.3)\n",
        "\n",
        "    # Ensures the generated SQL actually addresses the input question, using 0.5 i.e results should be at least moderately relevant\n",
        "    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
        "\n",
        "    evaluation_results = evaluate(dataset, [\n",
        "        exact_match_metric,\n",
        "        hallucination_metric,\n",
        "        answer_relevancy_metric\n",
        "        ])\n",
        "\n",
        "    pass_rates = aggregate_metric_pass_rates(evaluation_results.test_results)\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(\"=====================\")\n",
        "    for metric, pass_rate in pass_rates.items():\n",
        "        print(f\"{metric}: {pass_rate * 100:.2f}% pass rate\")\n",
        "\n",
        "    return evaluation_results, pass_rates"
      ],
      "metadata": {
        "id": "RJ_wARozNj82"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"Generate a valid SQL query for the following natural language instruction:\n",
        "{query}\n",
        "Only generate SQL code and nothing else.\"\"\"\n",
        "\n",
        "# Run evaluation\n",
        "results, pass_rates = run_evaluation(df, llm, ZERO_SHOT_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FvLWS5ni1vJk",
        "outputId": "764b9a20-8c77-4e38-f01e-2864da9e0d60"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating evaluation dataset\n",
            "\n",
            " Running evaluation metrics\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mBase Metric Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Base Metric Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 15 test case(s) in parallel: |██████████|100% (15/15) [Time Taken: 00:14,  1.00test case/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output is fully aligned with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response was perfectly relevant, directly addressing the question with no irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of employees.\n",
            "  - actual output: ```sql\n",
            "SELECT COUNT(*) AS total_employees FROM employees;\n",
            "```\n",
            "  - expected output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - context: ['Show the total number of employees.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the request with relevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the second highest salary in the organization.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) AS SecondHighestSalary\n",
            "FROM employees\n",
            "WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "  - context: ['Display the second highest salary in the organization.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output aligns perfectly with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show departments that have more than 10 employees.\n",
            "  - actual output: ```sql\n",
            "SELECT department_id, department_name\n",
            "FROM departments\n",
            "WHERE (SELECT COUNT(*) FROM employees WHERE employees.department_id = departments.department_id) > 10;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\n",
            "  - context: ['Show departments that have more than 10 employees.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output is fully aligned with the provided context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input with no irrelevant statements. Fantastic job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names and emails of staff in the IT department.\n",
            "  - actual output: ```sql\n",
            "SELECT name, email FROM staff WHERE department = 'IT';\n",
            "```\n",
            "  - expected output: SELECT name, email FROM employees WHERE department = 'IT';\n",
            "  - context: ['List the names and emails of staff in the IT department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output aligns perfectly with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Get the average age of all managers.\n",
            "  - actual output: ```sql\n",
            "SELECT AVG(age) AS average_age\n",
            "FROM employees\n",
            "WHERE position = 'manager';\n",
            "```\n",
            "  - expected output: SELECT AVG(age) FROM employees WHERE position = 'Manager';\n",
            "  - context: ['Get the average age of all managers.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions and the actual output aligns perfectly with the context., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly aligns with the input request and contains no irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the department that has the highest average employee salary.\n",
            "  - actual output: ```sql\n",
            "SELECT department, AVG(salary) AS average_salary\n",
            "FROM employees\n",
            "GROUP BY department\n",
            "ORDER BY average_salary DESC\n",
            "LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\n",
            "  - context: ['Display the department that has the highest average employee salary.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output perfectly aligns with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is completely relevant, directly addressing the question without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which product has the least quantity in stock?\n",
            "  - actual output: ```sql\n",
            "SELECT product_name, quantity_in_stock \n",
            "FROM products \n",
            "ORDER BY quantity_in_stock ASC \n",
            "LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\n",
            "  - context: ['Which product has the least quantity in stock?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context and there are no contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and directly addresses the input request without any irrelevant information. Great job staying on point!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the average order value for each customer.\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id, AVG(order_value) AS average_order_value\n",
            "FROM orders\n",
            "GROUP BY customer_id;\n",
            "```\n",
            "  - expected output: SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\n",
            "  - context: ['Find the average order value for each customer.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions and the actual output fully aligns with the provided context., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and directly addresses the question without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which customers have spent more than $1000 in total?\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id\n",
            "FROM orders\n",
            "GROUP BY customer_id\n",
            "HAVING SUM(total_amount) > 1000;\n",
            "```\n",
            "  - expected output: SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\n",
            "  - context: ['Which customers have spent more than $1000 in total?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input without any irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names of employees who do not manage anyone.\n",
            "  - actual output: ```sql\n",
            "SELECT name \n",
            "FROM employees \n",
            "WHERE id NOT IN (SELECT manager_id FROM employees WHERE manager_id IS NOT NULL);\n",
            "```\n",
            "  - expected output: SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);\n",
            "  - context: ['List the names of employees who do not manage anyone.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context and there are no contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input query without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List employees who joined after 2015 and work in the Sales department.\n",
            "  - actual output: ```sql\n",
            "SELECT * FROM employees \n",
            "WHERE join_date > '2015-01-01' \n",
            "AND department = 'Sales';\n",
            "```\n",
            "  - expected output: SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\n",
            "  - context: ['List employees who joined after 2015 and work in the Sales department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output aligns perfectly with the provided context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant and directly addresses the input request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List all products that have never been ordered.\n",
            "  - actual output: ```sql\n",
            "SELECT p.*\n",
            "FROM products p\n",
            "LEFT JOIN orders o ON p.product_id = o.product_id\n",
            "WHERE o.order_id IS NULL;\n",
            "```\n",
            "  - expected output: SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\n",
            "  - context: ['List all products that have never been ordered.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ❌ Hallucination (score: 1.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no factual alignments, and there is a significant contradiction in the actual output regarding the calculation of orders from the current date minus 7 days, which indicates a complete mismatch with the context., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output precisely addresses the request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of orders placed each day last week.\n",
            "  - actual output: ```sql\n",
            "SELECT DATE(order_date) AS order_day, COUNT(*) AS total_orders\n",
            "FROM orders\n",
            "WHERE order_date >= CURDATE() - INTERVAL 7 DAY AND order_date < CURDATE()\n",
            "GROUP BY order_day\n",
            "ORDER BY order_day;\n",
            "```\n",
            "  - expected output: SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\n",
            "  - context: ['Show the total number of orders placed each day last week.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context, without any contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the highest salary in the Finance department.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) \n",
            "FROM employees \n",
            "WHERE department = 'Finance';\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - context: ['Find the highest salary in the Finance department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant, with no irrelevant statements present. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What are the titles of the top 5 selling books?\n",
            "  - actual output: ```sql\n",
            "SELECT title \n",
            "FROM books \n",
            "ORDER BY sales DESC \n",
            "LIMIT 5;\n",
            "```\n",
            "  - expected output: SELECT title FROM books ORDER BY sales DESC LIMIT 5;\n",
            "  - context: ['What are the titles of the top 5 selling books?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Base Metric: 0.00% pass rate\n",
            "Hallucination: 93.33% pass rate\n",
            "Answer Relevancy: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Base Metric: 0.00% pass rate\n",
            "Hallucination: 93.33% pass rate\n",
            "Answer Relevancy: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Evaluation Results:\n",
            "=====================\n",
            "Base Metric: 0.00% pass rate\n",
            "Hallucination: 93.33% pass rate\n",
            "Answer Relevancy: 100.00% pass rate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good job evaluating the LLM and Prompt on the entire dataset that we have created! If you notice it isn't that good at this task still. What do you think we can do to improve the evaluation performance? What issues do you see with the way we perform evaluations at this point of time?\n",
        "\n",
        "\n",
        "Following are some observations with current evaluation:\n",
        "\n",
        "- **Exact Match metric** has 0% pass rate because it's too strict\n",
        "- **Hallucination metric** has 93.33% pass rate  but failed on `Show the total number of orders placed each day last week` which is a False positive since the only difference is use of **CURDATE** and **CURRENT_DATE** between actual and expected query\n",
        "- **Answer relevancy metric** has 100% pass rate which could be considered as less strict metric as it will only focus on the relevancy part of query but not on the execution part of it"
      ],
      "metadata": {
        "id": "7GMJSp2C8jVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Validating LLM Outputs using Guardrails\n",
        "\n",
        "When building our LLM Application one difficult thing is the fact that LLM's are non-deterministic in their outputs. We are able to control different parameters within our LLM to mitigate this to a certain extent such as:\n",
        "1. temperature / top_p\n",
        "2. seed\n",
        "\n",
        "You can read more about these parameters from OpenAI's documentation [here](https://platform.openai.com/docs/api-reference/chat/create).\n",
        "\n",
        "But ensuring consistent outputs is still a problem. *This is where having guardrails comes into play.*\n",
        "\n",
        "> Guardrails = safety controls for LLMs\n",
        "\n",
        "Just like guardrails help prevent cars from going off the road, they can also help out with making sure LLM outputs are in line with set expectations.\n",
        "\n",
        "![Road guardrails](https://drive.google.com/uc?id=1KZCfJs9Sf6ExqpphnTa4G-AaisxFBRWG)\n",
        "\n",
        "Guardrails can take many different forms which differ in complexity:\n",
        "1. **Security Guardrails**, ensuring there isn't any PII data in the response\n",
        "2. **Compliance Guardrails**, ensuring that competitors are not talked about\n",
        "3. **Safety Guardrails**, ensuring that there are not toxic responses\n",
        "4. **Structural Guardrails**, ensuring that the response matches a specific JSON schema\n",
        "\n",
        "\n",
        "A simple Guardrail for this situation could be to ensure that there are **no default error results incorporated within the product description**.\n",
        "\n",
        "In our application we will also be communicating via REST APIs which leverage JSON to send and receive data between different applications.\n",
        "\n",
        "**Frontend JSON sent to the Backend:**\n",
        "```json\n",
        "{\n",
        "    \"prompt\": \"Write a SQL query to find the names and email addresses of all users in the 'users' table who joined after January 1, 2020.\",\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_tokens\": 100\n",
        "}\n",
        "```\n",
        "**Backend Response JSON sent back to the Frontend:**\n",
        "```json\n",
        "{\n",
        "  \"id\": \"cmpl-XYZ456\",\n",
        "  \"object\": \"text_completion\",\n",
        "  \"created\": 1616517999,\n",
        "  \"model\": \"gpt-3.5-turbo\",\n",
        "  \"prediction\": {\n",
        "    \"text\": \"SELECT name, email FROM users WHERE join_date > '2020-01-01';\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "The response from OpenAI will be used to populate the `prediction` field within the main Backend Response where we will have an object that contains a `text` field which will then be used to contain the actual output from OpenAI.\n",
        "\n",
        "In this scenario we would initially look for a **Structural Guardrail** that ensures that the text response from the LLM:\n",
        "1. *is a string*\n",
        "2. *is not empty*\n",
        "\n",
        "A great library for this would be [Guardrails.ai](https://www.guardrailsai.com/) which allows us to leverage [Pydantic](https://docs.pydantic.dev/latest/) in order to easily define validators for our LLM responses.\n"
      ],
      "metadata": {
        "id": "rPmPTQssNDUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!guardrails configure --token $GUARDRAILS_TOKEN"
      ],
      "metadata": {
        "id": "ZSEG6jxaaJFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf46697-e12c-42d0-b35c-8e2aa480e4a9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Enable anonymous metrics reporting? [Y/n]: Y\n",
            "Do you wish to use remote inferencing? [Y/n]: Y\n",
            "\n",
            "            Login successful.\n",
            "\n",
            "            Get started by installing our RegexMatch validator:\n",
            "            https://hub.guardrailsai.com/validator/guardrails_ai/regex_match\n",
            "\n",
            "            You can install it by running:\n",
            "            guardrails hub install hub://guardrails/regex_match\n",
            "\n",
            "            Find more validators at https://hub.guardrailsai.com\n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class LLMResponse(BaseModel):\n",
        "  generated_sql: str = Field(description=\"Generated SQL from LLM\")"
      ],
      "metadata": {
        "id": "JUf5Y8tizDdW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from rich import print\n",
        "from guardrails import Guard\n",
        "\n",
        "# Add in your prompt here from before along with the query\n",
        "# NOTE: gr.complete_json_suffix_v2 comes from guardrail where it will inject\n",
        "#       additional tokens related to enforcing the Pydantic Model, to the main prompt.\n",
        "ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE = \"\"\"\n",
        "Generate a valid SQL query for the following natural language instructions:\n",
        "\n",
        "Instructions:\n",
        "- Only generate a SELECT query\n",
        "- End the query with a semicolon\n",
        "- Do not include any explanations, only the SQL query\n",
        "\n",
        "${query}\n",
        "\n",
        "${gr.complete_json_suffix_v3}\n",
        "\"\"\"\n",
        "\n",
        "guard = Guard.from_pydantic(output_class=LLMResponse, prompt=ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE)\n",
        "\n",
        "raw_llm_output, validated_output, *rest = guard(\n",
        "    llm_api=openai.chat.completions.create,\n",
        "    model=MODEL_NAME,\n",
        "    prompt_params={\n",
        "        \"query\": SAMPLE_QUERY\n",
        "    },\n",
        ")\n"
      ],
      "metadata": {
        "id": "6E10suedyZwv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows what was sent to the LLM, including the guardrails formatting\n",
        "print(guard.history.last.prompt)"
      ],
      "metadata": {
        "id": "PRoWerMjB1um",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "141a2f46-48c2-4458-a116-346687b1cddd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Generate a valid SQL query for the following natural language instructions:\n",
              "\n",
              "Instructions:\n",
              "- Only generate a SELECT query\n",
              "- End the query with a semicolon\n",
              "- Do not include any explanations, only the SQL query\n",
              "\n",
              "$\u001b[1m{\u001b[0mquery\u001b[1m}\u001b[0m\n",
              "\n",
              "$\u001b[1m{\u001b[0mgr.complete_json_suffix_v3\u001b[1m}\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Generate a valid SQL query for the following natural language instructions:\n",
              "\n",
              "Instructions:\n",
              "- Only generate a SELECT query\n",
              "- End the query with a semicolon\n",
              "- Do not include any explanations, only the SQL query\n",
              "\n",
              "$<span style=\"font-weight: bold\">{</span>query<span style=\"font-weight: bold\">}</span>\n",
              "\n",
              "$<span style=\"font-weight: bold\">{</span>gr.complete_json_suffix_v3<span style=\"font-weight: bold\">}</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shows the raw LLM response\n",
        "print(raw_llm_output)"
      ],
      "metadata": {
        "id": "utarYL7rTmcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e97fef1f-c50a-483f-eda1-c5d19d193e67"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m\"generated_sql\"\u001b[0m:\u001b[32m\"SELECT name FROM employees ORDER BY salary DESC LIMIT 1;\"\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">\"generated_sql\"</span>:<span style=\"color: #008000; text-decoration-color: #008000\">\"SELECT name FROM employees ORDER BY salary DESC LIMIT 1;\"</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows llm response object with generated_sql, it was successful\n",
        "print(validated_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "otmh76saBwHz",
        "outputId": "980fe3f2-be3b-4935-9d40-b45dbf96250e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m'generated_sql'\u001b[0m: \u001b[32m'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_sql'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can view the history of our prompts in a nice tree like format\n",
        "print(guard.history.last.tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 934
        },
        "id": "NMhpA6jtDlLJ",
        "outputId": "473ab4b0-b56c-4600-f2f6-55ed1f6669ec"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Logs\n",
              "└── ╭────────────────────────────────────────────────── Step 0 ───────────────────────────────────────────────────╮\n",
              "    │ \u001b[48;2;240;248;255m╭─\u001b[0m\u001b[48;2;240;248;255m───────────────────────────────────────────────\u001b[0m Prompt \u001b[48;2;240;248;255m────────────────────────────────────────────────\u001b[0m\u001b[48;2;240;248;255m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mGenerate a valid SQL query for the following natural language instructions:\u001b[0m\u001b[48;2;240;248;255m                            \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mInstructions:\u001b[0m\u001b[48;2;240;248;255m                                                                                          \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- Only generate a SELECT query\u001b[0m\u001b[48;2;240;248;255m                                                                         \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- End the query with a semicolon\u001b[0m\u001b[48;2;240;248;255m                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- Do not include any explanations, only the SQL query\u001b[0m\u001b[48;2;240;248;255m                                                  \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mSelect the name of the employee who has the highest salary\u001b[0m\u001b[48;2;240;248;255m                                             \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mGiven below is JSON Schema that describes the information to extract from this document and the tags to\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mextract it into.\u001b[0m\u001b[48;2;240;248;255m                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m{\"properties\": {\"generated_sql\": {\"description\": \"Generated SQL from LLM\", \"title\": \"Generated Sql\", \u001b[0m\u001b[48;2;240;248;255m  \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m\"type\": \"string\"}}, \"required\": [\"generated_sql\"], \"type\": \"object\", \"title\": \"LLMResponse\"}\u001b[0m\u001b[48;2;240;248;255m           \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mONLY return a valid JSON object (no other text is necessary). The JSON MUST conform to the JSON Schema,\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mincluding any types and format requests e.g. requests for lists, objects and specific types. Be correct\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mand concise. If you are unsure anywhere, try your best guess.\u001b[0m\u001b[48;2;240;248;255m                                          \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mHere are examples of simple (JSON Schema, JSON) pairs that show the expected behavior:\u001b[0m\u001b[48;2;240;248;255m                 \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- `{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}` => \u001b[0m\u001b[48;2;240;248;255m       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{'foo': 'example one'}`\u001b[0m\u001b[48;2;240;248;255m                                                                               \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- \u001b[0m\u001b[48;2;240;248;255m                                                                                                     \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}}}\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m}` => `{\"bar\": ['STRING ONE', 'STRING TWO']}`\u001b[0m\u001b[48;2;240;248;255m                                                          \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- \u001b[0m\u001b[48;2;240;248;255m                                                                                                     \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{\"type\":\"object\",\"properties\":{\"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"c\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mapitalize two-words\"},\"index\":{\"type\":\"integer\",\"format\":\"1-indexed\"}}}}}` => `{'baz': {'foo': 'Some \u001b[0m\u001b[48;2;240;248;255m  \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255mString', 'index': 1}}`\u001b[0m\u001b[48;2;240;248;255m                                                                                 \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m- \u001b[0m\u001b[48;2;240;248;255m                                                                                                     \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}},\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m\"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}}}` => \u001b[0m\u001b[48;2;240;248;255m  \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m`{'bar': ['STRING ONE', 'STRING TWO'], 'baz': {'foo': 'example one'}}`\u001b[0m\u001b[48;2;240;248;255m                                 \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m│\u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m                                                                                                       \u001b[0m\u001b[48;2;240;248;255m \u001b[0m\u001b[48;2;240;248;255m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;248;255m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m╭─\u001b[0m\u001b[48;2;255;240;242m────────────────────────────────────────────\u001b[0m Instructions \u001b[48;2;255;240;242m─────────────────────────────────────────────\u001b[0m\u001b[48;2;255;240;242m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m│\u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242mYou are a helpful assistant, able to express yourself purely through JSON, strictly and precisely \u001b[0m\u001b[48;2;255;240;242m     \u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242m│\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m│\u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242madhering to the provided JSON schema.\u001b[0m\u001b[48;2;255;240;242m                                                                  \u001b[0m\u001b[48;2;255;240;242m \u001b[0m\u001b[48;2;255;240;242m│\u001b[0m │\n",
              "    │ \u001b[48;2;255;240;242m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m╭─\u001b[0m\u001b[48;2;231;223;235m───────────────────────────────────────────\u001b[0m Message History \u001b[48;2;231;223;235m───────────────────────────────────────────\u001b[0m\u001b[48;2;231;223;235m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m│\u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235mNo message history.\u001b[0m\u001b[48;2;231;223;235m                                                                                    \u001b[0m\u001b[48;2;231;223;235m \u001b[0m\u001b[48;2;231;223;235m│\u001b[0m │\n",
              "    │ \u001b[48;2;231;223;235m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m╭─\u001b[0m\u001b[48;2;245;245;220m───────────────────────────────────────────\u001b[0m Raw LLM Output \u001b[48;2;245;245;220m────────────────────────────────────────────\u001b[0m\u001b[48;2;245;245;220m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m│\u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m{\"generated_sql\":\"SELECT name FROM employees ORDER BY salary DESC LIMIT 1;\"}\u001b[0m\u001b[48;2;245;245;220m                           \u001b[0m\u001b[48;2;245;245;220m \u001b[0m\u001b[48;2;245;245;220m│\u001b[0m │\n",
              "    │ \u001b[48;2;245;245;220m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m╭─\u001b[0m\u001b[48;2;240;255;240m──────────────────────────────────────────\u001b[0m Validated Output \u001b[48;2;240;255;240m───────────────────────────────────────────\u001b[0m\u001b[48;2;240;255;240m─╮\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m│\u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m{'generated_sql': 'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'}\u001b[0m\u001b[48;2;240;255;240m                          \u001b[0m\u001b[48;2;240;255;240m \u001b[0m\u001b[48;2;240;255;240m│\u001b[0m │\n",
              "    │ \u001b[48;2;240;255;240m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m │\n",
              "    ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Logs\n",
              "└── ╭────────────────────────────────────────────────── Step 0 ───────────────────────────────────────────────────╮\n",
              "    │ <span style=\"background-color: #f0f8ff\">╭────────────────────────────────────────────────</span> Prompt <span style=\"background-color: #f0f8ff\">─────────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Generate a valid SQL query for the following natural language instructions:                             │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Instructions:                                                                                           │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ - Only generate a SELECT query                                                                          │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ - End the query with a semicolon                                                                        │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ - Do not include any explanations, only the SQL query                                                   │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Select the name of the employee who has the highest salary                                              │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Given below is JSON Schema that describes the information to extract from this document and the tags to │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ extract it into.                                                                                        │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ {\"properties\": {\"generated_sql\": {\"description\": \"Generated SQL from LLM\", \"title\": \"Generated Sql\",    │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ \"type\": \"string\"}}, \"required\": [\"generated_sql\"], \"type\": \"object\", \"title\": \"LLMResponse\"}            │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ ONLY return a valid JSON object (no other text is necessary). The JSON MUST conform to the JSON Schema, │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ including any types and format requests e.g. requests for lists, objects and specific types. Be correct │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ and concise. If you are unsure anywhere, try your best guess.                                           │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ Here are examples of simple (JSON Schema, JSON) pairs that show the expected behavior:                  │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ - `{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}` =&gt;         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{'foo': 'example one'}`                                                                                │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ -                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}}} │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ }` =&gt; `{\"bar\": ['STRING ONE', 'STRING TWO']}`                                                           │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ -                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{\"type\":\"object\",\"properties\":{\"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"c │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ apitalize two-words\"},\"index\":{\"type\":\"integer\",\"format\":\"1-indexed\"}}}}}` =&gt; `{'baz': {'foo': 'Some    │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ String', 'index': 1}}`                                                                                  │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ -                                                                                                       │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{\"type\":\"object\",\"properties\":{\"bar\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"format\":\"upper-case\"}}, │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ \"baz\":{\"type\":\"object\",\"properties\":{\"foo\":{\"type\":\"string\",\"format\":\"two-words lower-case\"}}}}}` =&gt;    │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│ `{'bar': ['STRING ONE', 'STRING TWO'], 'baz': {'foo': 'example one'}}`                                  │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">│                                                                                                         │</span> │\n",
              "    │ <span style=\"background-color: #f0f8ff\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">╭─────────────────────────────────────────────</span> Instructions <span style=\"background-color: #fff0f2\">──────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">│ You are a helpful assistant, able to express yourself purely through JSON, strictly and precisely       │</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">│ adhering to the provided JSON schema.                                                                   │</span> │\n",
              "    │ <span style=\"background-color: #fff0f2\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">╭────────────────────────────────────────────</span> Message History <span style=\"background-color: #e7dfeb\">────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">│ No message history.                                                                                     │</span> │\n",
              "    │ <span style=\"background-color: #e7dfeb\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">╭────────────────────────────────────────────</span> Raw LLM Output <span style=\"background-color: #f5f5dc\">─────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">│ {\"generated_sql\":\"SELECT name FROM employees ORDER BY salary DESC LIMIT 1;\"}                            │</span> │\n",
              "    │ <span style=\"background-color: #f5f5dc\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">╭───────────────────────────────────────────</span> Validated Output <span style=\"background-color: #f0fff0\">────────────────────────────────────────────╮</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">│ {'generated_sql': 'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'}                           │</span> │\n",
              "    │ <span style=\"background-color: #f0fff0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span> │\n",
              "    ╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Adding More Guardrails\n",
        "We have created a very basic Guardrail that leverages Pydantic Model's to ensure that our response fits a specifc format. Now let us try more advanced guardrails:\n",
        "- Try creating **Bug Free SQL Code** leveraging the following [example](https://www.guardrailsai.com/docs/examples/syntax_error_free_sql).\n",
        "- Feel free to add in more [guardrails via the Guardrails.ai Hub](https://hub.guardrailsai.com/) where you see fit for our use case. For example, we could ensure that there isn't any toxic language being returned from out LLM via using the [ToxicLanguage Validator](https://www.guardrailsai.com/docs/examples/toxic_language).\n",
        "- How should we handle validation failures? One way would be to leverage reasking, where we reask the LLM to fix the output from a validation check failure. Within Guardrails.ai there are corrective measures we can take to handle validation errors as outlined [here](https://www.guardrailsai.com/docs/how_to_guides/rail#%EF%B8%8F-specifying-corrective-actions).\n",
        "\n"
      ],
      "metadata": {
        "id": "KtDOZyPK2D4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! guardrails hub install hub://guardrails/valid_sql --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PisWlqX_TFA",
        "outputId": "de736485-e56b-4984-87e7-ab223f63ac2d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mvalid_sql...\u001b[0m\n",
            "✅Successfully installed guardrails/valid_sql!\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install sqlvalidator -q"
      ],
      "metadata": {
        "id": "44jurmSwADB4"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bug Free SQL Code"
      ],
      "metadata": {
        "id": "MS1vD_eXCYzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from rich import print\n",
        "from guardrails import Guard\n",
        "from guardrails.hub.guardrails.valid_sql.validator import ValidSQL\n",
        "\n",
        "ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE = \"\"\"\n",
        "Generate a valid SQL query for the following natural language instructions:\n",
        "\n",
        "Instructions:\n",
        "- Only generate a SELECT query\n",
        "- End the query with a semicolon\n",
        "- Do not include any explanations, only the SQL query\n",
        "\n",
        "${query}\n",
        "\n",
        "${gr.complete_json_suffix_v3}\n",
        "\"\"\"\n",
        "\n",
        "# Defining output class with the ValidSQL validator\n",
        "class LLMResponse(BaseModel):\n",
        "    generated_sql: str = Field(description=\"Generate SQL for the given natural language instruction.\", validators=[ValidSQL(on_fail=\"reask\")])\n",
        "\n",
        "\n",
        "guard = Guard.from_pydantic(output_class=LLMResponse, prompt=ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE)\n",
        "\n",
        "\n",
        "raw_llm_output, validated_output, *rest = guard(\n",
        "    llm_api=openai.chat.completions.create,\n",
        "    model=MODEL_NAME,\n",
        "    prompt_params={\n",
        "        \"query\": SAMPLE_QUERY\n",
        "    },\n",
        ")\n",
        "\n",
        "print(validated_output)"
      ],
      "metadata": {
        "id": "XQ-2FIj2NnaH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "980586b8-1b32-4d95-e6e7-4be06fb80d7b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m'generated_sql'\u001b[0m: \u001b[32m'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_sql'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! guardrails hub install hub://guardrails/toxic_language --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js6h9fBJBHvg",
        "outputId": "a6afd57f-6e68-4036-f93d-cd58b4d93003"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing hub:\u001b[35m/\u001b[0m\u001b[35m/guardrails/\u001b[0m\u001b[95mtoxic_language...\u001b[0m\n",
            "✅Successfully installed guardrails/toxic_language!\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " ### Toxic Language Validator"
      ],
      "metadata": {
        "id": "TqA-0gasCdHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import openai\n",
        "from rich import print\n",
        "from guardrails import Guard\n",
        "from guardrails.hub.guardrails.toxic_language.validator import ToxicLanguage\n",
        "\n",
        "\n",
        "ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE = \"\"\"\n",
        "Generate a valid SQL query for the following natural language instructions:\n",
        "\n",
        "Instructions:\n",
        "- Only generate a SELECT query\n",
        "- End the query with a semicolon\n",
        "- Do not include any explanations, only the SQL query\n",
        "\n",
        "${query}\n",
        "\n",
        "${gr.complete_json_suffix_v3}\n",
        "\"\"\"\n",
        "\n",
        "# Defining output class with the ValidSQL validator\n",
        "class LLMResponse(BaseModel):\n",
        "    generated_sql: str = Field(description=\"Generate SQL for the given natural language instruction.\", validators=[ToxicLanguage(on_fail=\"reask\")])\n",
        "\n",
        "\n",
        "guard = Guard.from_pydantic(output_class=LLMResponse, prompt=ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE)\n",
        "\n",
        "\n",
        "raw_llm_output, validated_output, *rest = guard(\n",
        "    llm_api=openai.chat.completions.create,\n",
        "    model=MODEL_NAME,\n",
        "    prompt_params={\n",
        "        \"query\": SAMPLE_QUERY\n",
        "    },\n",
        ")\n",
        "\n",
        "print(validated_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "cLnP35YjA7ho",
        "outputId": "fa878cfd-a764-435e-dcf3-c26d05e1e16f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m'generated_sql'\u001b[0m: \u001b[32m'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_sql'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How should we handle validation failures?"
      ],
      "metadata": {
        "id": "F46_jQphCqev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To automatically correct outputs containing toxic language, we will set on_fail='fix'\n",
        "import openai\n",
        "from rich import print\n",
        "from guardrails import Guard\n",
        "from guardrails.hub.guardrails.toxic_language.validator import ToxicLanguage\n",
        "\n",
        "\n",
        "ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE = \"\"\"\n",
        "Generate a valid SQL query for the following natural language instructions:\n",
        "\n",
        "Instructions:\n",
        "- Only generate a SELECT query\n",
        "- End the query with a semicolon\n",
        "- Do not include any explanations, only the SQL query\n",
        "\n",
        "${query}\n",
        "\n",
        "${gr.complete_json_suffix_v3}\n",
        "\"\"\"\n",
        "\n",
        "# Defining output class with the ValidSQL validator\n",
        "class LLMResponse(BaseModel):\n",
        "    generated_sql: str = Field(description=\"Generate SQL for the given natural language instruction.\", validators=[ToxicLanguage(on_fail=\"fix\")])\n",
        "\n",
        "\n",
        "guard = Guard.from_pydantic(output_class=LLMResponse, prompt=ZERO_SHOT_PROMPT_GUARDRAILS_TEMPLATE)\n",
        "\n",
        "\n",
        "raw_llm_output, validated_output, *rest = guard(\n",
        "    llm_api=openai.chat.completions.create,\n",
        "    model=MODEL_NAME,\n",
        "    prompt_params={\n",
        "        \"query\": SAMPLE_QUERY\n",
        "    },\n",
        ")\n",
        "\n",
        "print(validated_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "JD50ejipCp5N",
        "outputId": "e49244d5-6ffb-4192-fefc-a6cf5e5a04db"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m'generated_sql'\u001b[0m: \u001b[32m'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'generated_sql'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SELECT name FROM employees ORDER BY salary DESC LIMIT 1;'</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job adding in robust guardrails, next we can now focus on making sure our system performs at peak performance. Let us try improving the results of the model using Prompt Engineering techniques making sure that we compare results from the baseline with the new improvements. Feel free to go wild and experiment with all kinds of techniques for this section!\n",
        "\n",
        "## TODO: Evaluation Driven Development(EDD)\n",
        "\n",
        "EDD is similar to [Test Driven Development(TDD)](https://martinfowler.com/bliki/TestDrivenDevelopment.html) except we apply it to building LLM-powered applications, where our test cases in this case are derived from the Golden dataset that we created earlier and the goal here is to iteratively ensure that our LLM application can achieve a **100% pass rate** across all the examples.\n",
        "\n",
        "The main idea here is that if we currently have a 60% pass rate on the exact match metric, we should take an iterative approach towards increasing the pass rate. For example, the LLM could be failing on the following query `Display the department that has the highest average employee salary.`. In this section we will try to tune the prompt or other parts to ensure that the model is able to correctly generate the ground truth for it.\n",
        "\n",
        "NOTE: As we mentioned before some items from the dataset could result in false negatives since multiple implementations are possible for the same query.\n",
        "\n",
        "Here are a couple of things we can try out:\n",
        "- Try out Few-Shot Prompting and [other techniques](https://www.promptingguide.ai/techniques) and see how it impacts our evaluation performance.\n",
        "- Try tuning the parameters used for a model.\n",
        "- Try out different LLM models from OpenAI, Anthropic, etc. if you have API keys for them.\n",
        "\n",
        "You will have to setup an EvaluationDataset per each comparison since each run would have a different `actual_output` used within the LLMTestCase.\n",
        "```python\n",
        "from deepeval.evaluate import aggregate_metric_pass_rates\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "dataset_run_1 = EvaluationDataset(test_cases=[...])\n",
        "dataset_run_2 = EvaluationDataset(test_cases=[...])\n",
        "\n",
        "evaluation_results_run_1 = evaluate(dataset_run_1, [exact_match_metric])\n",
        "pass_rate_run_1 = aggregate_metric_pass_rates(evaluation_results_run_1.test_results)\n",
        "\n",
        "evaluation_results_run_2 = evaluate(dataset_run_2, [exact_match_metric])\n",
        "pass_rate_run_2 = aggregate_metric_pass_rates(evaluation_results_run_2.test_results)\n",
        "\n",
        "assert pass_rate_run_2.get(\"ExactMatch\") > pass_rate_run_1.get(\"ExactMatch\"), \\\n",
        "  \"Run 2 does not perform better than Run 1\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "VgWBsMZ-Ex96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics.base_metric import BaseMetric\n",
        "from deepeval.scorer import Scorer\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "from langchain.schema import HumanMessage\n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric\n",
        "from deepeval.evaluate import aggregate_metric_pass_rates\n",
        "\n",
        "\n",
        "class ExactMatchMetric(BaseMetric):\n",
        "    def __init__(self, threshold=0.0, async_mode=True):\n",
        "        self.threshold = threshold\n",
        "        self.async_mode = async_mode\n",
        "\n",
        "    def _base_measure(self, test_case: LLMTestCase):\n",
        "        self.success = Scorer.exact_match_score(\n",
        "            test_case.actual_output,\n",
        "            test_case.expected_output\n",
        "        )\n",
        "        self.score = 1 if self.success else 0\n",
        "        return self.score\n",
        "\n",
        "    def measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    async def a_measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    def is_successful(self):\n",
        "        return bool(self.success)\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return \"ExactMatch\"\n",
        "\n",
        "\n",
        "def create_evaluation_dataset(df, llm, prompt_template):\n",
        "    test_cases = []\n",
        "    for _, row in df.iterrows():\n",
        "        prompt = prompt_template.format(query=row['Query'])\n",
        "        result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "        actual_output = result.generations[0][0].text\n",
        "        test_case = LLMTestCase(\n",
        "            input=row['Query'],\n",
        "            actual_output=actual_output,\n",
        "            expected_output=row['Ground Truth'],\n",
        "            context=[row['Query']]\n",
        "        )\n",
        "        test_cases.append(test_case)\n",
        "    return EvaluationDataset(test_cases=test_cases)\n",
        "\n",
        "\n",
        "\n",
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"Generate a valid SQL query for the following natural language instruction:\n",
        "{query}\n",
        "Only generate SQL code and nothing else.\"\"\"\n",
        "\n",
        "\n",
        "FEW_SHOT_PROMPT_TEMPLATE = \"\"\"\n",
        "Here are some examples of natural language instructions and their SQL equivalents:\n",
        "1. Instruction: \"Show the name and salary of employees who earn more than $100,000.\"\n",
        "   SQL: \"SELECT name, salary FROM employees WHERE salary > 100000;\"\n",
        "2. Instruction: \"List the departments along with the number of employees in each.\"\n",
        "   SQL: \"SELECT department, COUNT(*) FROM employees GROUP BY department;\"\n",
        "\n",
        "Now generate a valid SQL query for the following instruction:\n",
        "{query}\n",
        "Only provide the SQL code.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def run_evaluation(df, llm, prompt_template, prompt_name):\n",
        "    print(f\"Creating evaluation dataset for {prompt_name} prompting...\")\n",
        "    dataset = create_evaluation_dataset(df, llm, prompt_template)\n",
        "\n",
        "    print(f\"\\nRunning evaluation metrics for {prompt_name} prompting...\")\n",
        "    exact_match_metric = ExactMatchMetric()\n",
        "    hallucination_metric = HallucinationMetric(threshold=0.3)\n",
        "    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
        "\n",
        "    evaluation_results = evaluate(dataset, [\n",
        "        exact_match_metric,\n",
        "        hallucination_metric,\n",
        "        answer_relevancy_metric\n",
        "    ])\n",
        "\n",
        "    pass_rates = aggregate_metric_pass_rates(evaluation_results.test_results)\n",
        "\n",
        "    print(f\"\\nEvaluation Results for {prompt_name} Prompting:\")\n",
        "    print(\"===================================================\")\n",
        "    for metric, pass_rate in pass_rates.items():\n",
        "        print(f\"{metric}: {pass_rate * 100:.2f}% pass rate\")\n",
        "\n",
        "    return evaluation_results, pass_rates"
      ],
      "metadata": {
        "id": "fBl8t9QZNovv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_zero_shot, pass_rates_zero_shot = run_evaluation(df, llm, ZERO_SHOT_PROMPT_TEMPLATE, \"Zero-Shot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ki5BEw3NHO77",
        "outputId": "8c998899-9960-4073-a4f3-fb9174cd378c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating evaluation dataset for Zero-Shot prompting\u001b[33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating evaluation dataset for Zero-Shot prompting<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Running evaluation metrics for Zero-Shot prompting\u001b[33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Running evaluation metrics for Zero-Shot prompting<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mBase Metric Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Base Metric Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 15 test case(s) in parallel: |██████████|100% (15/15) [Time Taken: 00:17,  1.16s/test case]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and focused, providing exactly the information requested without any irrelevant details. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names and emails of staff in the IT department.\n",
            "  - actual output: ```sql\n",
            "SELECT name, email FROM staff WHERE department = 'IT';\n",
            "```\n",
            "  - expected output: SELECT name, email FROM employees WHERE department = 'IT';\n",
            "  - context: ['List the names and emails of staff in the IT department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context without any contradictions, indicating no hallucination., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response was perfectly relevant and directly addressed the question without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which product has the least quantity in stock?\n",
            "  - actual output: ```sql\n",
            "SELECT product_name \n",
            "FROM products \n",
            "ORDER BY quantity_in_stock ASC \n",
            "LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\n",
            "  - context: ['Which product has the least quantity in stock?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output perfectly aligns with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the input without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of employees.\n",
            "  - actual output: ```sql\n",
            "SELECT COUNT(*) AS total_employees FROM employees;\n",
            "```\n",
            "  - expected output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - context: ['Show the total number of employees.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output aligns perfectly with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the highest salary in the Finance department.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) \n",
            "FROM employees \n",
            "WHERE department = 'Finance';\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - context: ['Find the highest salary in the Finance department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output aligns perfectly with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the query without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show departments that have more than 10 employees.\n",
            "  - actual output: ```sql\n",
            "SELECT department_id, department_name\n",
            "FROM departments\n",
            "WHERE (SELECT COUNT(*) FROM employees WHERE employees.department_id = departments.department_id) > 10;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\n",
            "  - context: ['Show departments that have more than 10 employees.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output SQL query is perfectly aligned with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant to the input request without any irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the second highest salary in the organization.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) AS SecondHighestSalary\n",
            "FROM employees\n",
            "WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "  - context: ['Display the second highest salary in the organization.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output aligns perfectly with the provided context and there are no contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant and fully addresses the input without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names of employees who do not manage anyone.\n",
            "  - actual output: ```sql\n",
            "SELECT name \n",
            "FROM employees \n",
            "WHERE id NOT IN (SELECT manager_id FROM employees WHERE manager_id IS NOT NULL);\n",
            "```\n",
            "  - expected output: SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);\n",
            "  - context: ['List the names of employees who do not manage anyone.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ❌ Hallucination (score: 1.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there is a clear contradiction between the actual output and the expected context regarding the retrieval of orders, indicating a complete misalignment., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input without any irrelevant statements. Great job staying focused and relevant!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of orders placed each day last week.\n",
            "  - actual output: ```sql\n",
            "SELECT DATE(order_date) AS order_day, COUNT(*) AS total_orders\n",
            "FROM orders\n",
            "WHERE order_date >= CURDATE() - INTERVAL 7 DAY AND order_date < CURDATE()\n",
            "GROUP BY order_day\n",
            "ORDER BY order_day;\n",
            "```\n",
            "  - expected output: SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\n",
            "  - context: ['Show the total number of orders placed each day last week.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context, accurately listing the top 5 selling books by ordering them by sales in descending order without any contradictions., error: None)\n",
            "  - ❌ Answer Relevancy (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the output consists entirely of SQL query elements, which do not directly answer the question about the titles of the top-selling books., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What are the titles of the top 5 selling books?\n",
            "  - actual output: ```sql\n",
            "SELECT title \n",
            "FROM books \n",
            "ORDER BY sales DESC \n",
            "LIMIT 5;\n",
            "```\n",
            "  - expected output: SELECT title FROM books ORDER BY sales DESC LIMIT 5;\n",
            "  - context: ['What are the titles of the top 5 selling books?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context and there are no contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the question without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which customers have spent more than $1000 in total?\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id\n",
            "FROM orders\n",
            "GROUP BY customer_id\n",
            "HAVING SUM(total_amount) > 1000;\n",
            "```\n",
            "  - expected output: SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\n",
            "  - context: ['Which customers have spent more than $1000 in total?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the provided context without any contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input with no irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List all products that have never been ordered.\n",
            "  - actual output: ```sql\n",
            "SELECT p.*\n",
            "FROM products p\n",
            "LEFT JOIN orders o ON p.product_id = o.product_id\n",
            "WHERE o.order_id IS NULL;\n",
            "```\n",
            "  - expected output: SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\n",
            "  - context: ['List all products that have never been ordered.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the provided context without any contradictions or deviations., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant and directly addresses the input query. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the average order value for each customer.\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id, AVG(order_value) AS average_order_value\n",
            "FROM orders\n",
            "GROUP BY customer_id;\n",
            "```\n",
            "  - expected output: SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\n",
            "  - context: ['Find the average order value for each customer.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context and there are no contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant and directly addresses the input request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the department that has the highest average employee salary.\n",
            "  - actual output: ```sql\n",
            "SELECT department, AVG(salary) AS average_salary\n",
            "FROM employees\n",
            "GROUP BY department\n",
            "ORDER BY average_salary DESC\n",
            "LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\n",
            "  - context: ['Display the department that has the highest average employee salary.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output perfectly aligns with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the input without any irrelevant information. Great job maintaining focus on the task!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List employees who joined after 2015 and work in the Sales department.\n",
            "  - actual output: ```sql\n",
            "SELECT * FROM employees \n",
            "WHERE join_date > '2015-01-01' \n",
            "AND department = 'Sales';\n",
            "```\n",
            "  - expected output: SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\n",
            "  - context: ['List employees who joined after 2015 and work in the Sales department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions, and the actual output accurately aligns with the context by calculating the average age of all managers., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is completely relevant and perfectly addresses the input request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Get the average age of all managers.\n",
            "  - actual output: ```sql\n",
            "SELECT AVG(age) AS average_age\n",
            "FROM employees\n",
            "WHERE position = 'manager';\n",
            "```\n",
            "  - expected output: SELECT AVG(age) FROM employees WHERE position = 'Manager';\n",
            "  - context: ['Get the average age of all managers.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Base Metric: 0.00% pass rate\n",
            "Hallucination: 93.33% pass rate\n",
            "Answer Relevancy: 93.33% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Base Metric: 0.00% pass rate\n",
            "Hallucination: 93.33% pass rate\n",
            "Answer Relevancy: 93.33% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Evaluation Results for Zero-Shot Prompting:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Evaluation Results for Zero-Shot Prompting:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "===================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">===================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Base Metric: \u001b[1;36m0.00\u001b[0m% pass rate\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Base Metric: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>% pass rate\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Hallucination: \u001b[1;36m93.33\u001b[0m% pass rate\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hallucination: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93.33</span>% pass rate\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Answer Relevancy: \u001b[1;36m93.33\u001b[0m% pass rate\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Answer Relevancy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93.33</span>% pass rate\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_few_shot, pass_rates_few_shot = run_evaluation(df, llm, FEW_SHOT_PROMPT_TEMPLATE, \"Few-Shot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KYfMtsjLHYMI",
        "outputId": "9b748710-77f6-491e-e02d-5f61e95b44ed"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating evaluation dataset for Few-Shot prompting\u001b[33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating evaluation dataset for Few-Shot prompting<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Running evaluation metrics for Few-Shot prompting\u001b[33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Running evaluation metrics for Few-Shot prompting<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mBase Metric Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Base Metric Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 15 test case(s) in parallel: |██████████|100% (15/15) [Time Taken: 00:16,  1.10s/test case]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the provided context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant, addressing the input question directly without any irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What are the titles of the top 5 selling books?\n",
            "  - actual output: ```sql\n",
            "SELECT title FROM books ORDER BY sales DESC LIMIT 5;\n",
            "```\n",
            "  - expected output: SELECT title FROM books ORDER BY sales DESC LIMIT 5;\n",
            "  - context: ['What are the titles of the top 5 selling books?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the provided context without any contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant and focused on addressing the request without any irrelevant information., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the second highest salary in the organization.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "  - context: ['Display the second highest salary in the organization.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context without any contradictions, indicating no hallucination., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant to the input, with no irrelevant statements present. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which customers have spent more than $1000 in total?\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(amount) > 1000;\n",
            "```\n",
            "  - expected output: SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\n",
            "  - context: ['Which customers have spent more than $1000 in total?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context by accurately calculating the average order value for each customer, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and directly addresses the input request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the average order value for each customer.\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\n",
            "```\n",
            "  - expected output: SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\n",
            "  - context: ['Find the average order value for each customer.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the provided context without any contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and directly addresses the request without including any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Get the average age of all managers.\n",
            "  - actual output: ```sql\n",
            "SELECT AVG(age) FROM employees WHERE position = 'manager';\n",
            "```\n",
            "  - expected output: SELECT AVG(age) FROM employees WHERE position = 'Manager';\n",
            "  - context: ['Get the average age of all managers.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions and the actual output fully aligns with the provided context., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the department that has the highest average employee salary.\n",
            "  - actual output: ```sql\n",
            "SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\n",
            "  - context: ['Display the department that has the highest average employee salary.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context without any contradictions, indicating no hallucination., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input with no irrelevant information. Great job staying precise and focused!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names of employees who do not manage anyone.\n",
            "  - actual output: ```sql\n",
            "SELECT name FROM employees WHERE id NOT IN (SELECT manager_id FROM employees WHERE manager_id IS NOT NULL);\n",
            "```\n",
            "  - expected output: SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);\n",
            "  - context: ['List the names of employees who do not manage anyone.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output is fully aligned with the context and there are no contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and directly addresses the request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names and emails of staff in the IT department.\n",
            "  - actual output: ```sql\n",
            "SELECT name, email FROM employees WHERE department = 'IT';\n",
            "```\n",
            "  - expected output: SELECT name, email FROM employees WHERE department = 'IT';\n",
            "  - context: ['List the names and emails of staff in the IT department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the given context by correctly providing a SQL query that retrieves and groups the total number of orders placed each day for the last 7 days, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the input request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of orders placed each day last week.\n",
            "  - actual output: ```sql\n",
            "SELECT DATE(order_date) AS order_day, COUNT(*) AS total_orders \n",
            "FROM orders \n",
            "WHERE order_date >= CURDATE() - INTERVAL 7 DAY \n",
            "GROUP BY order_day;\n",
            "```\n",
            "  - expected output: SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\n",
            "  - context: ['Show the total number of orders placed each day last week.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output aligns perfectly with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly and succinctly addressed the query without any irrelevant information. Great job on staying focused!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of employees.\n",
            "  - actual output: ```sql\n",
            "SELECT COUNT(*) FROM employees;\n",
            "```\n",
            "  - expected output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - context: ['Show the total number of employees.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the provided context without any contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the input without any irrelevant information. Great job keeping it relevant and focused!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List employees who joined after 2015 and work in the Sales department.\n",
            "  - actual output: ```sql\n",
            "SELECT * FROM employees WHERE join_date > '2015-01-01' AND department = 'Sales';\n",
            "```\n",
            "  - expected output: SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\n",
            "  - context: ['List employees who joined after 2015 and work in the Sales department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output aligns perfectly with the provided context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and directly addresses the task of finding the highest salary in the Finance department. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the highest salary in the Finance department.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - context: ['Find the highest salary in the Finance department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions between the actual output and the context, indicating complete factual alignment., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant to the input, with no irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which product has the least quantity in stock?\n",
            "  - actual output: ```sql\n",
            "SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\n",
            "  - context: ['Which product has the least quantity in stock?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context and there are no contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant and addresses the input request without any irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List all products that have never been ordered.\n",
            "  - actual output: ```sql\n",
            "SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\n",
            "```\n",
            "  - expected output: SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\n",
            "  - context: ['List all products that have never been ordered.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context and there are no contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show departments that have more than 10 employees.\n",
            "  - actual output: ```sql\n",
            "SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\n",
            "  - context: ['Show departments that have more than 10 employees.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Base Metric: 0.00% pass rate\n",
            "Hallucination: 100.00% pass rate\n",
            "Answer Relevancy: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Base Metric: 0.00% pass rate\n",
            "Hallucination: 100.00% pass rate\n",
            "Answer Relevancy: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Evaluation Results for Few-Shot Prompting:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Evaluation Results for Few-Shot Prompting:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "===================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">===================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Base Metric: \u001b[1;36m0.00\u001b[0m% pass rate\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Base Metric: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>% pass rate\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Hallucination: \u001b[1;36m100.00\u001b[0m% pass rate\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hallucination: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.00</span>% pass rate\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Answer Relevancy: \u001b[1;36m100.00\u001b[0m% pass rate\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Answer Relevancy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.00</span>% pass rate\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nComparison of Prompting Techniques:\")\n",
        "for metric in pass_rates_zero_shot.keys():\n",
        "    zero_shot_rate = pass_rates_zero_shot[metric] * 100\n",
        "    few_shot_rate = pass_rates_few_shot[metric] * 100\n",
        "    print(f\"{metric} Pass Rate:\")\n",
        "    print(f\"  Zero-Shot: {zero_shot_rate:.2f}%\")\n",
        "    print(f\"  Few-Shot: {few_shot_rate:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "DADKCFYEHf8e",
        "outputId": "8ec13223-d3d0-4689-bd86-ea8d67e381d1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Comparison of Prompting Techniques:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Comparison of Prompting Techniques:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Base Metric Pass Rate:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Base Metric Pass Rate:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Zero-Shot: \u001b[1;36m0.00\u001b[0m%\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Zero-Shot: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>%\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Few-Shot: \u001b[1;36m0.00\u001b[0m%\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Few-Shot: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>%\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Hallucination Pass Rate:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hallucination Pass Rate:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Zero-Shot: \u001b[1;36m93.33\u001b[0m%\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Zero-Shot: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93.33</span>%\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Few-Shot: \u001b[1;36m100.00\u001b[0m%\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Few-Shot: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.00</span>%\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Answer Relevancy Pass Rate:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Answer Relevancy Pass Rate:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Zero-Shot: \u001b[1;36m93.33\u001b[0m%\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Zero-Shot: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93.33</span>%\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Few-Shot: \u001b[1;36m100.00\u001b[0m%\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Few-Shot: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.00</span>%\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Higher pass rates for hallucination with few shot prompting as compared to zero shot prompting"
      ],
      "metadata": {
        "id": "_Lq0F3IcIAKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning the parameters used for a model."
      ],
      "metadata": {
        "id": "Nm32c8gmJnpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics.base_metric import BaseMetric\n",
        "from deepeval.scorer import Scorer\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "from langchain.schema import HumanMessage\n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric\n",
        "from deepeval.evaluate import aggregate_metric_pass_rates\n",
        "\n",
        "\n",
        "class ExactMatchMetric(BaseMetric):\n",
        "    def __init__(self, threshold=0.0, async_mode=True):\n",
        "        self.threshold = threshold\n",
        "        self.async_mode = async_mode\n",
        "\n",
        "    def _base_measure(self, test_case: LLMTestCase):\n",
        "        self.success = Scorer.exact_match_score(\n",
        "            test_case.actual_output,\n",
        "            test_case.expected_output\n",
        "        )\n",
        "        self.score = 1 if self.success else 0\n",
        "        return self.score\n",
        "\n",
        "    def measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    async def a_measure(self, test_case: LLMTestCase):\n",
        "        self._base_measure(test_case=test_case)\n",
        "\n",
        "    def is_successful(self):\n",
        "        return bool(self.success)\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return \"ExactMatch\"\n",
        "\n",
        "\n",
        "def create_evaluation_dataset(df, llm, prompt_template, llm_config):\n",
        "    test_cases = []\n",
        "    for _, row in df.iterrows():\n",
        "        prompt = prompt_template.format(query=row['Query'])\n",
        "        result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "        actual_output = result.generations[0][0].text\n",
        "        test_case = LLMTestCase(\n",
        "            input=row['Query'],\n",
        "            actual_output=actual_output,\n",
        "            expected_output=row['Ground Truth'],\n",
        "            context=[row['Query']]\n",
        "        )\n",
        "        test_cases.append(test_case)\n",
        "    return EvaluationDataset(test_cases=test_cases)\n",
        "\n",
        "\n",
        "\n",
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"Generate a valid SQL query for the following natural language instruction:\n",
        "{query}\n",
        "Only generate SQL code and nothing else.\"\"\"\n",
        "\n",
        "\n",
        "def run_evaluation(df, llm, prompt_template, llm_config):\n",
        "    print(\"Creating evaluation dataset...\")\n",
        "    dataset = create_evaluation_dataset(df, llm, prompt_template, llm_config)\n",
        "\n",
        "    print(\"\\nRunning evaluation metrics...\")\n",
        "    exact_match_metric = ExactMatchMetric()\n",
        "    hallucination_metric = HallucinationMetric(threshold=0.3)\n",
        "    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n",
        "\n",
        "    evaluation_results = evaluate(dataset, [\n",
        "        exact_match_metric,\n",
        "        hallucination_metric,\n",
        "        answer_relevancy_metric\n",
        "    ])\n",
        "\n",
        "    pass_rates = aggregate_metric_pass_rates(evaluation_results.test_results)\n",
        "\n",
        "    print(\"\\nEvaluation Results:\")\n",
        "    print(\"======================\")\n",
        "    for metric, pass_rate in pass_rates.items():\n",
        "        print(f\"{metric}: {pass_rate * 100:.2f}% pass rate\")\n",
        "\n",
        "    return evaluation_results, pass_rates\n",
        "\n",
        "\n",
        "# Configuration 1: Lower temperature for more deterministic output\n",
        "llm_config_1 = {\n",
        "    \"temperature\": 0.0,\n",
        "    \"max_tokens\": 150,\n",
        "}\n",
        "\n",
        "# Configuration 2: Higher temperature for more creative output\n",
        "llm_config_2 = {\n",
        "    \"temperature\": 0.8,\n",
        "    \"max_tokens\": 150\n",
        "}"
      ],
      "metadata": {
        "id": "-dw1pC4tJCTA"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_config_1, pass_rates_config_1 = run_evaluation(df, llm, ZERO_SHOT_PROMPT_TEMPLATE, llm_config_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G9yh-7v5K3B9",
        "outputId": "17abad8a-5724-4479-db3f-61d86113b900"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating evaluation dataset\u001b[33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating evaluation dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Running evaluation metrics\u001b[33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Running evaluation metrics<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mBase Metric Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Base Metric Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 15 test case(s) in parallel: |██████████|100% (15/15) [Time Taken: 00:19,  1.30s/test case]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output aligns perfectly with the context without any contradictions, indicating no hallucination., error: None)\n",
            "  - ❌ Answer Relevancy (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the output provided a SQL query, which does not directly answer the question about the titles of the top 5 selling books., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What are the titles of the top 5 selling books?\n",
            "  - actual output: ```sql\n",
            "SELECT title \n",
            "FROM books \n",
            "ORDER BY sales DESC \n",
            "LIMIT 5;\n",
            "```\n",
            "  - expected output: SELECT title FROM books ORDER BY sales DESC LIMIT 5;\n",
            "  - context: ['What are the titles of the top 5 selling books?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions and the actual output aligns perfectly with the context by providing the correct SQL query., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the input without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of employees.\n",
            "  - actual output: ```sql\n",
            "SELECT COUNT(*) AS total_employees FROM employees;\n",
            "```\n",
            "  - expected output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - context: ['Show the total number of employees.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the provided context without any contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input with no irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the highest salary in the Finance department.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) \n",
            "FROM employees \n",
            "WHERE department = 'Finance';\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - context: ['Find the highest salary in the Finance department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context and there are no contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly matched the request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names and emails of staff in the IT department.\n",
            "  - actual output: ```sql\n",
            "SELECT name, email FROM staff WHERE department = 'IT';\n",
            "```\n",
            "  - expected output: SELECT name, email FROM employees WHERE department = 'IT';\n",
            "  - context: ['List the names and emails of staff in the IT department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List all products that have never been ordered.\n",
            "  - actual output: ```sql\n",
            "SELECT * FROM products \n",
            "WHERE product_id NOT IN (SELECT DISTINCT product_id FROM orders);\n",
            "```\n",
            "  - expected output: SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\n",
            "  - context: ['List all products that have never been ordered.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the provided context and there are no contradictions, indicating no hallucination., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the input without any irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names of employees who do not manage anyone.\n",
            "  - actual output: ```sql\n",
            "SELECT name \n",
            "FROM employees \n",
            "WHERE id NOT IN (SELECT manager_id FROM employees WHERE manager_id IS NOT NULL);\n",
            "```\n",
            "  - expected output: SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);\n",
            "  - context: ['List the names of employees who do not manage anyone.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output aligns with the context without any contradictions, indicating no hallucination., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant and directly addresses the task of finding the average order value for each customer. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the average order value for each customer.\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id, AVG(order_value) AS average_order_value\n",
            "FROM orders\n",
            "GROUP BY customer_id;\n",
            "```\n",
            "  - expected output: SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\n",
            "  - context: ['Find the average order value for each customer.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output aligns perfectly with the context without any contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant with no irrelevant statements, directly addressing the input request., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show departments that have more than 10 employees.\n",
            "  - actual output: ```sql\n",
            "SELECT department_id, department_name\n",
            "FROM departments\n",
            "WHERE (SELECT COUNT(*) FROM employees WHERE employees.department_id = departments.department_id) > 10;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\n",
            "  - context: ['Show departments that have more than 10 employees.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the provided context without any contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and directly addresses the question without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which product has the least quantity in stock?\n",
            "  - actual output: ```sql\n",
            "SELECT product_name \n",
            "FROM products \n",
            "ORDER BY quantity_in_stock ASC \n",
            "LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\n",
            "  - context: ['Which product has the least quantity in stock?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ❌ Hallucination (score: 1.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the actual output contradicts the context by incorrectly including orders from the current day, which deviates from the intended query logic of showing orders for each day last week., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of orders placed each day last week.\n",
            "  - actual output: ```sql\n",
            "SELECT DATE(order_date) AS order_day, COUNT(*) AS total_orders\n",
            "FROM orders\n",
            "WHERE order_date >= CURDATE() - INTERVAL 7 DAY\n",
            "GROUP BY order_day\n",
            "ORDER BY order_day;\n",
            "```\n",
            "  - expected output: SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\n",
            "  - context: ['Show the total number of orders placed each day last week.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output perfectly aligns with the provided context without any contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and addresses the input without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List employees who joined after 2015 and work in the Sales department.\n",
            "  - actual output: ```sql\n",
            "SELECT * FROM employees \n",
            "WHERE join_date > '2015-01-01' \n",
            "AND department = 'Sales';\n",
            "```\n",
            "  - expected output: SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\n",
            "  - context: ['List employees who joined after 2015 and work in the Sales department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context and there are no contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response was perfectly relevant and directly addressed the request without any irrelevant information., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Get the average age of all managers.\n",
            "  - actual output: ```sql\n",
            "SELECT AVG(age) AS average_age\n",
            "FROM employees\n",
            "WHERE position = 'manager';\n",
            "```\n",
            "  - expected output: SELECT AVG(age) FROM employees WHERE position = 'Manager';\n",
            "  - context: ['Get the average age of all managers.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context without any contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addressed the request by providing the second highest salary with no irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the second highest salary in the organization.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) AS SecondHighestSalary\n",
            "FROM employees\n",
            "WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "  - context: ['Display the second highest salary in the organization.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions, and the actual output perfectly aligns with the context by accurately reflecting the intended SQL query operation., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input without any irrelevant information. Great job on maintaining complete focus!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the department that has the highest average employee salary.\n",
            "  - actual output: ```sql\n",
            "SELECT department, AVG(salary) AS average_salary\n",
            "FROM employees\n",
            "GROUP BY department\n",
            "ORDER BY average_salary DESC\n",
            "LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\n",
            "  - context: ['Display the department that has the highest average employee salary.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context and there are no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response directly addresses the input without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which customers have spent more than $1000 in total?\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id\n",
            "FROM orders\n",
            "GROUP BY customer_id\n",
            "HAVING SUM(total_amount) > 1000;\n",
            "```\n",
            "  - expected output: SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\n",
            "  - context: ['Which customers have spent more than $1000 in total?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Base Metric: 0.00% pass rate\n",
            "Hallucination: 93.33% pass rate\n",
            "Answer Relevancy: 93.33% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Base Metric: 0.00% pass rate\n",
            "Hallucination: 93.33% pass rate\n",
            "Answer Relevancy: 93.33% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Evaluation Results:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Evaluation Results:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "======================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Base Metric: \u001b[1;36m0.00\u001b[0m% pass rate\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Base Metric: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>% pass rate\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Hallucination: \u001b[1;36m93.33\u001b[0m% pass rate\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hallucination: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93.33</span>% pass rate\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Answer Relevancy: \u001b[1;36m93.33\u001b[0m% pass rate\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Answer Relevancy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93.33</span>% pass rate\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_config_2, pass_rates_config_2 = run_evaluation(df, llm, ZERO_SHOT_PROMPT_TEMPLATE, llm_config_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RhyZ6ARrLT-l",
        "outputId": "a7207ef7-90b9-4754-e0f2-af88e130c931"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating evaluation dataset\u001b[33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Creating evaluation dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Running evaluation metrics\u001b[33m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Running evaluation metrics<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mBase Metric Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing \u001b[0m\u001b[3;38;2;55;65;81mNone\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Base Metric Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using </span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">None</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 15 test case(s) in parallel: |██████████|100% (15/15) [Time Taken: 00:16,  1.11s/test case]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context without any contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and concise, directly addressing the request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of employees.\n",
            "  - actual output: ```sql\n",
            "SELECT COUNT(*) AS total_employees FROM employees;\n",
            "```\n",
            "  - expected output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - context: ['Show the total number of employees.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context and contains no contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input with no irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List employees who joined after 2015 and work in the Sales department.\n",
            "  - actual output: ```sql\n",
            "SELECT * FROM employees \n",
            "WHERE join_date > '2015-01-01' \n",
            "AND department = 'Sales';\n",
            "```\n",
            "  - expected output: SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\n",
            "  - context: ['List employees who joined after 2015 and work in the Sales department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions between the actual output and the context, and the output aligns perfectly with the given factual context., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and directly answers the question without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What are the titles of the top 5 selling books?\n",
            "  - actual output: ```sql\n",
            "SELECT title \n",
            "FROM books \n",
            "ORDER BY sales DESC \n",
            "LIMIT 5;\n",
            "```\n",
            "  - expected output: SELECT title FROM books ORDER BY sales DESC LIMIT 5;\n",
            "  - context: ['What are the titles of the top 5 selling books?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions and the actual output aligns perfectly with the context., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the answer is perfectly relevant and directly addresses the question without any irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Get the average age of all managers.\n",
            "  - actual output: ```sql\n",
            "SELECT AVG(age) AS average_age\n",
            "FROM employees\n",
            "WHERE position = 'manager';\n",
            "```\n",
            "  - expected output: SELECT AVG(age) FROM employees WHERE position = 'Manager';\n",
            "  - context: ['Get the average age of all managers.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output aligns perfectly with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response was perfectly relevant and directly addressed the task of finding the highest salary in the Finance department. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the highest salary in the Finance department.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) \n",
            "FROM employees \n",
            "WHERE department = 'Finance';\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - context: ['Find the highest salary in the Finance department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output perfectly aligns with the context without any contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show departments that have more than 10 employees.\n",
            "  - actual output: ```sql\n",
            "SELECT department_id, department_name\n",
            "FROM departments\n",
            "WHERE (SELECT COUNT(*) FROM employees WHERE employees.department_id = departments.department_id) > 10;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\n",
            "  - context: ['Show departments that have more than 10 employees.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output perfectly aligns with the context and there are no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant to the input with no irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names of employees who do not manage anyone.\n",
            "  - actual output: ```sql\n",
            "SELECT name \n",
            "FROM employees \n",
            "WHERE id NOT IN (SELECT manager_id FROM employees WHERE manager_id IS NOT NULL);\n",
            "```\n",
            "  - expected output: SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);\n",
            "  - context: ['List the names of employees who do not manage anyone.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because there are no contradictions between the actual output and the context, indicating full alignment., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response is perfectly relevant and concise, directly addressing the question without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which product has the least quantity in stock?\n",
            "  - actual output: ```sql\n",
            "SELECT product_name, quantity_in_stock \n",
            "FROM products \n",
            "ORDER BY quantity_in_stock ASC \n",
            "LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\n",
            "  - context: ['Which product has the least quantity in stock?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ❌ Hallucination (score: 1.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the actual output contradicts the provided context by including orders from the current day, while it should exclude them as per the context., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the input without any irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of orders placed each day last week.\n",
            "  - actual output: ```sql\n",
            "SELECT DATE(order_date) AS order_day, COUNT(*) AS total_orders\n",
            "FROM orders\n",
            "WHERE order_date >= CURDATE() - INTERVAL 7 DAY AND order_date < CURDATE()\n",
            "GROUP BY order_day\n",
            "ORDER BY order_day;\n",
            "```\n",
            "  - expected output: SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\n",
            "  - context: ['Show the total number of orders placed each day last week.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the provided context without any contradictions, indicating no hallucination., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names and emails of staff in the IT department.\n",
            "  - actual output: ```sql\n",
            "SELECT name, email FROM staff WHERE department = 'IT';\n",
            "```\n",
            "  - expected output: SELECT name, email FROM employees WHERE department = 'IT';\n",
            "  - context: ['List the names and emails of staff in the IT department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output perfectly aligns with the context and there are no contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the question without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which customers have spent more than $1000 in total?\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id\n",
            "FROM orders\n",
            "GROUP BY customer_id\n",
            "HAVING SUM(total_amount) > 1000;\n",
            "```\n",
            "  - expected output: SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\n",
            "  - context: ['Which customers have spent more than $1000 in total?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output aligns perfectly with the context, showing no contradictions., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the department that has the highest average employee salary.\n",
            "  - actual output: ```sql\n",
            "SELECT department, AVG(salary) AS average_salary\n",
            "FROM employees\n",
            "GROUP BY department\n",
            "ORDER BY average_salary DESC\n",
            "LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\n",
            "  - context: ['Display the department that has the highest average employee salary.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context without any contradictions, indicating no hallucination., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input without any irrelevant statements. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List all products that have never been ordered.\n",
            "  - actual output: ```sql\n",
            "SELECT p.*\n",
            "FROM products p\n",
            "LEFT JOIN orders o ON p.product_id = o.product_id\n",
            "WHERE o.order_id IS NULL;\n",
            "```\n",
            "  - expected output: SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\n",
            "  - context: ['List all products that have never been ordered.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output is fully aligned with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant and directly addresses the request without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the second highest salary in the organization.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) AS SecondHighestSalary\n",
            "FROM employees\n",
            "WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "  - context: ['Display the second highest salary in the organization.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ❌ Base Metric (score: 0.0, threshold: 0.0, strict: False, evaluation model: None, reason: None, error: None)\n",
            "  - ✅ Hallucination (score: 0.0, threshold: 0.3, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the actual output fully aligns with the context, with no contradictions present., error: None)\n",
            "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addressed the request to find the average order value for each customer without any irrelevant information. Great job!, error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the average order value for each customer.\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id, AVG(order_value) AS average_order_value\n",
            "FROM orders\n",
            "GROUP BY customer_id;\n",
            "```\n",
            "  - expected output: SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\n",
            "  - context: ['Find the average order value for each customer.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Base Metric: 0.00% pass rate\n",
            "Hallucination: 93.33% pass rate\n",
            "Answer Relevancy: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Base Metric: 0.00% pass rate\n",
            "Hallucination: 93.33% pass rate\n",
            "Answer Relevancy: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Evaluation Results:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Evaluation Results:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "======================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">======================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Base Metric: \u001b[1;36m0.00\u001b[0m% pass rate\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Base Metric: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>% pass rate\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Hallucination: \u001b[1;36m93.33\u001b[0m% pass rate\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hallucination: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93.33</span>% pass rate\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Answer Relevancy: \u001b[1;36m100.00\u001b[0m% pass rate\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Answer Relevancy: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.00</span>% pass rate\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nComparison of Parameter Configurations:\")\n",
        "print(\"===========================================\")\n",
        "for metric in pass_rates_config_1.keys():\n",
        "    config_1_rate = pass_rates_config_1[metric] * 100\n",
        "    config_2_rate = pass_rates_config_2[metric] * 100\n",
        "    print(f\"{metric} Pass Rate:\")\n",
        "    print(f\"  Config 1: {config_1_rate:.2f}%\")\n",
        "    print(f\"  Config 2: {config_2_rate:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "lfW8Q0SJLYA7",
        "outputId": "cfb06f0e-033d-484d-b711-81980f277d59"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Comparison of Parameter Configurations:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Comparison of Parameter Configurations:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "===========================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">===========================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Base Metric Pass Rate:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Base Metric Pass Rate:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Config \u001b[1;36m1\u001b[0m: \u001b[1;36m0.00\u001b[0m%\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Config <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>%\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Config \u001b[1;36m2\u001b[0m: \u001b[1;36m0.00\u001b[0m%\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Config <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span>%\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Hallucination Pass Rate:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Hallucination Pass Rate:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Config \u001b[1;36m1\u001b[0m: \u001b[1;36m93.33\u001b[0m%\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Config <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93.33</span>%\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Config \u001b[1;36m2\u001b[0m: \u001b[1;36m93.33\u001b[0m%\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Config <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93.33</span>%\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Answer Relevancy Pass Rate:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Answer Relevancy Pass Rate:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Config \u001b[1;36m1\u001b[0m: \u001b[1;36m93.33\u001b[0m%\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Config <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93.33</span>%\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Config \u001b[1;36m2\u001b[0m: \u001b[1;36m100.00\u001b[0m%\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Config <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.00</span>%\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM conf 2 achieves a pass rate of 100%, while llm conf 1 has a slightly lower pass rate of 93.33% for **answer relevancy**"
      ],
      "metadata": {
        "id": "hiDSQ6pgLs8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Evaluating SQL Code Using a Stronger LLM\n",
        "\n",
        "Usually we would prioritise latency during inference using LLMs for most use cases, LLMs such as `gpt3.5-turbo` are much faster than `gpt4` variants while performing worse in general across benchmarks. What if we could also use a bigger and stronger LLM to aid with evaluations as well? There has been a rise of techniques doing just that where a smaller and faster LLM is evaluated by a bigger and stronger LLM. This process helps automate the evaluation part of any LLM-powered application! This is kinda of like having a dream within a dream just like in the movie *Inception*.\n",
        "\n",
        "![Dream within a dream from Inception](https://drive.google.com/uc?id=1EC7rNg_LCLjIJ9SRyQpGVgtplOHZllDm)\n",
        "\n",
        "\n",
        "**NOTE:** LLMs are very powerful and can help automate so many parts of the process but becareful not to be too reliant on them, at the end of the day they aren't perfect and hallucinate a lot. Therefore, its vital that you always keep a human-in-the-loop to verify things as a final seal of approval.\n",
        "\n",
        "\n",
        "### Introducing G-Eval\n",
        "\n",
        "[G-Eval](https://arxiv.org/abs/2303.16634) is a method that utilizes LLMs alongside a Chain-of-Thought (CoT) and form-filling approach to assess the outputs of LLMs. Initially, a task outline and evaluation parameters are provided to an LLM, which is then requested to create a CoT detailing the evaluation steps. For assessing the coherence of news summaries, the prompt, CoT, news article, and summary are merged, and the LLM is instructed to produce a score ranging from 1 to 5. Subsequently, the output token probabilities generated by the LLM are used to standardize this score, with a weighted summation taken as the ultimate outcome.\n",
        "\n",
        "It was observed that using GPT-4 as an evaluator resulted in a high Spearman correlation with human assessments (0.514), surpassing all former methods. In the realm of summarization, it exceeded all prior state-of-the-art (SOTA) evaluators in the SummEval benchmark, which evaluates coherence, consistency, fluency, and relevance.\n",
        "\n",
        "\n",
        "![G-Eval](https://drive.google.com/uc?id=1dq1BAjLY_Es5r-UiD96ZnFlxWTYcNv8g)\n",
        "\n",
        "deepeval already comes out of the box with support for using G-Eval [here](https://docs.confident-ai.com/docs/metrics-llm-evals) where you can very easily just create a new metric and run it against your EvaluationDataset:\n",
        "\n",
        "```python\n",
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCaseParams\n",
        "\n",
        "correctness_metric = GEval(\n",
        "    name=\"Correctness\",\n",
        "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
        "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
        "    evaluation_steps=[\n",
        "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
        "        \"You should also heavily penalize omission of detail\",\n",
        "        \"Vague language, or contradicting OPINIONS, are OK\"\n",
        "    ],\n",
        "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
        ")\n",
        "```\n",
        "Try creating a G-Eval metric for SQL Code Generation and running it through our EvaluationDataset like before.\n",
        "\n",
        "\n",
        "**NOTE:** The API Key provided to you only allows access to `gpt-4o-mini`, we can still use it either way but in the real world you would ideally use a more powerful model for this task. If you were to use `gpt-4o` as your base model then you probably don't have another *stronger* LLM to use therefore this also mimics that kind of constraint."
      ],
      "metadata": {
        "id": "pnh0hFV-HLK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval import evaluate\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.dataset import EvaluationDataset\n",
        "\n",
        "def create_evaluation_dataset(df, llm, prompt_template):\n",
        "    test_cases = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "\n",
        "        # Formatting prompt with the query from df\n",
        "        prompt = prompt_template.format(query=row['Query'])\n",
        "\n",
        "        # translating text to sql using llm\n",
        "        result = llm.generate([[HumanMessage(content=prompt)]])\n",
        "\n",
        "        actual_output = result.generations[0][0].text\n",
        "\n",
        "        # Creating test cases and appending\n",
        "        test_case = LLMTestCase(\n",
        "            input=row['Query'],\n",
        "            actual_output=actual_output,\n",
        "            expected_output=row['Ground Truth'],\n",
        "            context=[row['Query']]\n",
        "        )\n",
        "        test_cases.append(test_case)\n",
        "\n",
        "    return EvaluationDataset(test_cases=test_cases)\n",
        "\n",
        "\n",
        "ZERO_SHOT_PROMPT_TEMPLATE = \"\"\"Generate a valid SQL query for the following natural language instruction:\n",
        "{query}\n",
        "Only generate SQL code and nothing else.\"\"\""
      ],
      "metadata": {
        "id": "LFL3M_1eNrYW"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dataset = create_evaluation_dataset(df,llm,ZERO_SHOT_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "3pgbE2DCOTqb"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import GEval\n",
        "from deepeval.test_case import LLMTestCaseParams\n",
        "\n",
        "sql_correctness_metric = GEval(\n",
        "    name=\"SQLCorrectness\",\n",
        "    criteria=\"Evaluate whether the generated SQL query correctly implements the intended functionality as described in the input.\",\n",
        "    evaluation_steps=[\n",
        "        \"Ensure that the query syntax is correct and adheres to SQL standards.\",\n",
        "        \"Check for the use of appropriate SQL clauses and functions relevant to the task.\"\n",
        "    ],\n",
        "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
        ")\n"
      ],
      "metadata": {
        "id": "VgazsrdbRouA"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = evaluate(evaluation_dataset, [sql_correctness_metric])\n",
        "pass_rates = aggregate_metric_pass_rates(evaluation_results.test_results)\n",
        "\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(\"=====================\")\n",
        "for metric, pass_rate in pass_rates.items():\n",
        "    print(f\"{metric}: {pass_rate * 100:.2f}% pass rate\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JeBDvB_GOkPK",
        "outputId": "e73191a1-4aad-420a-ec33-fed6be2146f7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mSQLCorrectness \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">SQLCorrectness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating 15 test case(s) in parallel: |██████████|100% (15/15) [Time Taken: 00:07,  1.99test case/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 0.9999999999999998, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The SQL query correctly uses ORDER BY and LIMIT clauses to find the product with the least quantity in stock, adhering to SQL standards., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which product has the least quantity in stock?\n",
            "  - actual output: ```sql\n",
            "SELECT product_name \n",
            "FROM products \n",
            "ORDER BY quantity_in_stock ASC \n",
            "LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT product_name FROM products ORDER BY quantity_in_stock ASC LIMIT 1;\n",
            "  - context: ['Which product has the least quantity in stock?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 0.9979667646577666, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The SQL query correctly uses the MAX function and the WHERE clause to find the highest salary in the Finance department, adhering to SQL standards., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the highest salary in the Finance department.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) \n",
            "FROM employees \n",
            "WHERE department = 'Finance';\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE department = 'Finance';\n",
            "  - context: ['Find the highest salary in the Finance department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The SQL query correctly selects the 'name' and 'email' columns from the 'staff' table with a WHERE clause filtering for the 'IT' department, adhering to SQL standards and addressing the task., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names and emails of staff in the IT department.\n",
            "  - actual output: ```sql\n",
            "SELECT name, email FROM staff WHERE department = 'IT';\n",
            "```\n",
            "  - expected output: SELECT name, email FROM employees WHERE department = 'IT';\n",
            "  - context: ['List the names and emails of staff in the IT department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The SQL query correctly uses the SELECT, AVG function, and GROUP BY clause to calculate the average order value for each customer, adhering to SQL standards., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Find the average order value for each customer.\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id, AVG(order_value) AS average_order_value\n",
            "FROM orders\n",
            "GROUP BY customer_id;\n",
            "```\n",
            "  - expected output: SELECT customer_id, AVG(order_value) FROM orders GROUP BY customer_id;\n",
            "  - context: ['Find the average order value for each customer.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The query syntax is correct and uses appropriate SQL clauses and functions to show the total number of employees by using COUNT(*) and FROM employees., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of employees.\n",
            "  - actual output: ```sql\n",
            "SELECT COUNT(*) AS total_employees FROM employees;\n",
            "```\n",
            "  - expected output: SELECT COUNT(*) AS total_employees FROM employees;\n",
            "  - context: ['Show the total number of employees.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 0.9651354864666055, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The SQL query correctly uses the SELECT, WHERE, AND clauses to filter employees who joined after 2015 and are in the Sales department. The syntax adheres to SQL standards., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List employees who joined after 2015 and work in the Sales department.\n",
            "  - actual output: ```sql\n",
            "SELECT * FROM employees \n",
            "WHERE join_date > '2015-01-01' \n",
            "AND department = 'Sales';\n",
            "```\n",
            "  - expected output: SELECT * FROM employees WHERE year(joined_date) > 2015 AND department = 'Sales';\n",
            "  - context: ['List employees who joined after 2015 and work in the Sales department.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 0.9136295661126166, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The SQL query correctly uses a subquery to count employees per department and applies a WHERE clause to filter departments with more than 10 employees. However, it could be optimized using a JOIN and GROUP BY instead of a subquery., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show departments that have more than 10 employees.\n",
            "  - actual output: ```sql\n",
            "SELECT department_id, department_name\n",
            "FROM departments\n",
            "WHERE (SELECT COUNT(*) FROM employees WHERE employees.department_id = departments.department_id) > 10;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department HAVING COUNT(*) > 10;\n",
            "  - context: ['Show departments that have more than 10 employees.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The SQL query correctly uses the AVG function, applies it to the 'age' column, and includes a WHERE clause to filter for 'manager' positions, adhering to SQL standards., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Get the average age of all managers.\n",
            "  - actual output: ```sql\n",
            "SELECT AVG(age) AS average_age\n",
            "FROM employees\n",
            "WHERE position = 'manager';\n",
            "```\n",
            "  - expected output: SELECT AVG(age) FROM employees WHERE position = 'Manager';\n",
            "  - context: ['Get the average age of all managers.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The SQL query correctly uses SELECT, FROM, ORDER BY, and LIMIT clauses to retrieve the titles of the top 5 selling books, adhering to SQL standards., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What are the titles of the top 5 selling books?\n",
            "  - actual output: ```sql\n",
            "SELECT title \n",
            "FROM books \n",
            "ORDER BY sales DESC \n",
            "LIMIT 5;\n",
            "```\n",
            "  - expected output: SELECT title FROM books ORDER BY sales DESC LIMIT 5;\n",
            "  - context: ['What are the titles of the top 5 selling books?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 0.998201378926075, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The SQL query correctly uses the SELECT, DISTINCT, and WHERE clauses to find products not in any order, meeting the task requirements., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List all products that have never been ordered.\n",
            "  - actual output: ```sql\n",
            "SELECT * FROM products \n",
            "WHERE product_id NOT IN (SELECT DISTINCT product_id FROM orders);\n",
            "```\n",
            "  - expected output: SELECT * FROM products WHERE product_id NOT IN (SELECT product_id FROM orders);\n",
            "  - context: ['List all products that have never been ordered.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The query correctly uses SQL syntax to find the second highest salary by selecting the maximum salary from employees where the salary is less than the maximum salary., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the second highest salary in the organization.\n",
            "  - actual output: ```sql\n",
            "SELECT MAX(salary) AS SecondHighestSalary\n",
            "FROM employees\n",
            "WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "```\n",
            "  - expected output: SELECT MAX(salary) FROM employees WHERE salary < (SELECT MAX(salary) FROM employees);\n",
            "  - context: ['Display the second highest salary in the organization.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 0.9982013787512141, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The query uses correct SQL syntax, with appropriate clauses and functions including GROUP BY, ORDER BY, and LIMIT to find the department with the highest average salary., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Display the department that has the highest average employee salary.\n",
            "  - actual output: ```sql\n",
            "SELECT department, AVG(salary) AS average_salary\n",
            "FROM employees\n",
            "GROUP BY department\n",
            "ORDER BY average_salary DESC\n",
            "LIMIT 1;\n",
            "```\n",
            "  - expected output: SELECT department FROM employees GROUP BY department ORDER BY AVG(salary) DESC LIMIT 1;\n",
            "  - context: ['Display the department that has the highest average employee salary.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The SQL query correctly uses SELECT, GROUP BY, and HAVING clauses to find customers with a total spend over $1000, adhering to SQL syntax and task requirements., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Which customers have spent more than $1000 in total?\n",
            "  - actual output: ```sql\n",
            "SELECT customer_id\n",
            "FROM orders\n",
            "GROUP BY customer_id\n",
            "HAVING SUM(total_amount) > 1000;\n",
            "```\n",
            "  - expected output: SELECT customer_id FROM orders GROUP BY customer_id HAVING SUM(order_value) > 1000;\n",
            "  - context: ['Which customers have spent more than $1000 in total?']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 0.9982013786629006, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The SQL query correctly uses SELECT, FROM, and WHERE clauses to list employee names who do not manage anyone, adhering to SQL standards and using a subquery appropriately., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: List the names of employees who do not manage anyone.\n",
            "  - actual output: ```sql\n",
            "SELECT name \n",
            "FROM employees \n",
            "WHERE id NOT IN (SELECT manager_id FROM employees WHERE manager_id IS NOT NULL);\n",
            "```\n",
            "  - expected output: SELECT name FROM employees WHERE name NOT IN (SELECT manager_name FROM employees);\n",
            "  - context: ['List the names of employees who do not manage anyone.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ✅ SQLCorrectness (GEval) (score: 0.9011457687688333, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The SQL query correctly uses SELECT, FROM, WHERE, GROUP BY, and ORDER BY clauses to calculate the total number of orders placed each day over the last week. The use of DATE function and INTERVAL is appropriate. However, 'last week' could imply a specific week rather than the last 7 days, which slightly affects precision., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Show the total number of orders placed each day last week.\n",
            "  - actual output: ```sql\n",
            "SELECT DATE(order_date) AS order_day, COUNT(*) AS total_orders\n",
            "FROM orders\n",
            "WHERE order_date >= CURDATE() - INTERVAL 7 DAY AND order_date < CURDATE()\n",
            "GROUP BY order_day\n",
            "ORDER BY order_day;\n",
            "```\n",
            "  - expected output: SELECT order_date, COUNT(*) FROM orders WHERE order_date > CURRENT_DATE - INTERVAL '7 days' GROUP BY order_date;\n",
            "  - context: ['Show the total number of orders placed each day last week.']\n",
            "  - retrieval context: None\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "SQLCorrectness (GEval): 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
              "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
              "instead.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "SQLCorrectness (GEval): 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Evaluation Results:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "Evaluation Results:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "=====================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">=====================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "SQLCorrectness \u001b[1m(\u001b[0mGEval\u001b[1m)\u001b[0m: \u001b[1;36m100.00\u001b[0m% pass rate\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SQLCorrectness <span style=\"font-weight: bold\">(</span>GEval<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100.00</span>% pass rate\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great job on getting G-Eval to work with our dataset! What do you think about using LLMs to evaluate another LLM vs just using traditional Machine Learning metrics? What are the Pros/Cons of it and where do you think it would work better or worse in?\n",
        "\n",
        "**ANSWER**\n",
        "- Traditional ML metrics works better for structured tasks in metrics like Precision, Recall and AUC for assessment while LLMs generate open ended responses across an infinite output space.\n",
        "\n",
        "**Pros**\n",
        "- Semantic Understanding, Context Awareness\n",
        "- Scalabilty, Consistency and Flexibility\n",
        "\n",
        "**Cons**\n",
        "- Bias, Reliability, Resource Intensive and Verification Challenges"
      ],
      "metadata": {
        "id": "HC6hxGkHRuEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Extra Credit: Optional Tasks\n",
        "\n",
        "- Try out implementing the Execution Score from the Snowflake team and comparing the results of it to Exact Match Metric and G-Eval, this [blog](https://medium.com/snowflake/inside-snowflake-building-the-most-powerful-sql-llm-in-the-world-1a33b3ee0d37) from the Snowflake Team covers how you can use LLMs to assists with evaluation.\n",
        "- If you use SQL enough you would know that there are multiple ways in which we can answer a particular query especially if you had to update your ground truth earlier. With that you can try out other frameworks such as [sql-eval](https://github.com/defog-ai/sql-eval) which provides mo like we did before but more importanly also includes code execution into context when evaluating LLMs.\n",
        "- Guardrails do not just apply to the output from an LLM, instead we can also apply them to the actual input that a user feeds into the LLM. You can try out **Detecting Toxicity**, **Restricting the input to just be related to SQL code generation**, **Limiting the amount of tokens used**, etc.\n",
        "- Our BASIC_GROUND_TRUTH_DATASET is quite small, see how you can add in more examples using your own domain knowledge or you could even leverage an LLM to do so via synthetic data generation which is supported within deepeval [here](https://docs.confident-ai.com/docs/evaluation-datasets-synthetic-data). Remember not all users will put in queries that can be answered with SQL code. For example, if the user asks `Who will win the next US Presidential Election?` you definitely don't want to answer it at all!\n"
      ],
      "metadata": {
        "id": "UL-XI8PiNU1z"
      }
    }
  ]
}